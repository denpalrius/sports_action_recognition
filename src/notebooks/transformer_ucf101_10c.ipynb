{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sports Action Recognition Using a Video Vision Transformer Model with `UCF101 10 Sports actions`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-06T02:10:35.139379Z",
     "iopub.status.busy": "2024-11-06T02:10:35.138805Z",
     "iopub.status.idle": "2024-11-06T02:10:51.356809Z",
     "shell.execute_reply": "2024-11-06T02:10:51.355557Z",
     "shell.execute_reply.started": "2024-11-06T02:10:35.139326Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install kagglehub --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-11-06T02:10:51.359159Z",
     "iopub.status.busy": "2024-11-06T02:10:51.358754Z",
     "iopub.status.idle": "2024-11-06T02:11:09.254053Z",
     "shell.execute_reply": "2024-11-06T02:11:09.253134Z",
     "shell.execute_reply.started": "2024-11-06T02:10:51.359087Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import accuracy_score, top_k_accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "import imageio\n",
    "import cv2\n",
    "from IPython.display import Image\n",
    "\n",
    "import tensorflow as tf  # for data preprocessing only\n",
    "import keras\n",
    "from keras import layers, ops, models, optimizers, losses, metrics\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from keras.metrics import top_k_categorical_accuracy\n",
    "\n",
    "import kagglehub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download latest version of the ucf101-action-recognition dataset from Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = kagglehub.dataset_download(\"matthewjansen/ucf101-action-recognition\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-06T02:12:09.427435Z",
     "iopub.status.busy": "2024-11-06T02:12:09.427055Z",
     "iopub.status.idle": "2024-11-06T02:12:09.431825Z",
     "shell.execute_reply": "2024-11-06T02:12:09.430893Z",
     "shell.execute_reply.started": "2024-11-06T02:12:09.427384Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(\"Path to dataset files: \\n\", path)\n",
    "print(\"\\nFiles in dataset directory:\\n\", os.listdir(path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Class Definition**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sports_actions = [\n",
    "    \"SkyDiving\",\n",
    "    \"Biking\",\n",
    "    \"HorseRace\",\n",
    "    \"Surfing\",\n",
    "    \"TennisSwing\",\n",
    "    \"Punch\",\n",
    "    \"Basketball\",\n",
    "    \"JumpRope\",\n",
    "    \"Archery\",\n",
    "    \"Skiing\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utility to transform video paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(dataset_type):\n",
    "    dataset_path = os.path.join(path, f\"{dataset_type}.csv\")\n",
    "    dataset = pd.read_csv(dataset_path)\n",
    "\n",
    "    # Filter dataset to only include the specified sports actions\n",
    "    filtered_dataset = dataset[dataset[\"label\"].isin(sports_actions)]\n",
    "\n",
    "    return pd.DataFrame(\n",
    "        {\n",
    "            \"label\": filtered_dataset[\"label\"],\n",
    "            \"video_name\": filtered_dataset[\"clip_name\"],\n",
    "            \"rel_path\": filtered_dataset[\"clip_path\"],\n",
    "            \"video_path\": filtered_dataset[\"clip_path\"].apply(lambda x: f\"{path}{x}\"),\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the train and test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = load_dataset(\"train\")\n",
    "val_df = load_dataset(\"val\")\n",
    "test_df = load_dataset(\"test\")\n",
    "\n",
    "\n",
    "print(f\"Total videos for training: {len(train_df)}\")\n",
    "print(f\"Total videos for validation: {len(val_df)}\")\n",
    "print(f\"Total videos for testing: {len(test_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of unique classes in training set: \", len(train_df[\"label\"].unique()))\n",
    "print(\"Number of unique classes in validation set: \", len(val_df[\"label\"].unique()))\n",
    "print(\"Number of unique classes in test set: \", len(test_df[\"label\"].unique()))\n",
    "\n",
    "print(\"\\nLabels: \\n\", train_df[\"label\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configs and Hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting seed for reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42 \n",
    "os.environ[\"TF_CUDNN_DETERMINISTIC\"] = \"1\"\n",
    "\n",
    "keras.utils.set_random_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL\n",
    "MODEL_NAME = \"transformer_ucf101_10c\"\n",
    "NUM_CLASSES = len(sports_actions)\n",
    "\n",
    "# DATA\n",
    "FRAME_SIZE = 224\n",
    "FRAME_COUNT = 20\n",
    "# FRAME_COUNT = 64 TODO: Change to 64 for final training\n",
    "INPUT_SHAPE = (FRAME_COUNT, FRAME_SIZE, FRAME_SIZE, 3)  # (frames, height, width, channels) 32 X 224 X 224 X 3\n",
    "BATCH_SIZE = 32\n",
    "AUTO = tf.data.AUTOTUNE\n",
    "\n",
    "# TRAINING\n",
    "EPOCHS = 60\n",
    "LEARNING_RATE = 1e-4\n",
    "WEIGHT_DECAY = 1e-5\n",
    "\n",
    "# ViViT ARCHITECTURE\n",
    "LAYER_NORM_EPS = 1e-6\n",
    "PROJECTION_DIM = 128\n",
    "NUM_HEADS = 8 \n",
    "NUM_LAYERS = 8\n",
    "\n",
    "# TUBELET CONFIG\n",
    "PATCH_SIZE = (8, 8, 8)\n",
    "NUM_PATCHES = (INPUT_SHAPE[0] // PATCH_SIZE[0]) * (INPUT_SHAPE[1] // PATCH_SIZE[1]) * (INPUT_SHAPE[2] // PATCH_SIZE[2])\n",
    "# = (32//8) * (224//8) * (224//8)\n",
    "# = 4 * 28 * 28\n",
    "# = 3,136 patches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set path for model saving and retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = \"../../models\"\n",
    "pattern = re.compile(rf\"{MODEL_NAME}_v(\\d+)\\.keras\")\n",
    "\n",
    "existing_versions = [\n",
    "    int(pattern.search(f).group(1)) \n",
    "    for f in os.listdir(base_dir) if pattern.search(f)\n",
    "]\n",
    "version = max(existing_versions, default=0) + 1\n",
    "MODEL_PATH = os.path.join(base_dir, f\"{MODEL_NAME}_v{version}.keras\")\n",
    "\n",
    "print(f\"Model will be saved to: '{MODEL_PATH}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Video Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Review video category distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def review_class_distribution(dataset, dataset_name):\n",
    "    class_counts = dataset[\"label\"].value_counts()\n",
    "    return class_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get class distributions for each dataset\n",
    "train_class_counts = review_class_distribution(train_df, \"Train\")\n",
    "val_class_counts = review_class_distribution(val_df, \"Validation\")\n",
    "test_class_counts = review_class_distribution(test_df, \"Test\")\n",
    "\n",
    "# Create DataFrame for distribution and calculate average\n",
    "distribution_df = pd.DataFrame({\n",
    "    \"Train\": train_class_counts,\n",
    "    \"Validation\": val_class_counts,\n",
    "    \"Test\": test_class_counts\n",
    "}).fillna(0)\n",
    "\n",
    "distribution_df[\"Average\"] = distribution_df.mean(axis=1).round().astype(int)\n",
    "print(\"Combined average number of videos per class:\")\n",
    "print(distribution_df)\n",
    "\n",
    "# Plot the distribution\n",
    "plot_distribution_df = distribution_df.drop(columns=\"Average\")\n",
    "plot_distribution_df.plot(kind=\"bar\", figsize=(10, 5))\n",
    "plt.title(\"Class Distribution Comparison Across Train, Validation, and Test Sets\")\n",
    "plt.xlabel(\"Class Labels\")\n",
    "plt.ylabel(\"Number of Videos\")\n",
    "plt.legend(title=\"Dataset\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Review Video frame distribution "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Count the number of frames for each video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_frames_per_video(video_paths):\n",
    "    frame_counts = []\n",
    "\n",
    "    for video_path in video_paths: \n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        count = 0\n",
    "        \n",
    "        if len(frame_counts) > 10:\n",
    "            break\n",
    "        \n",
    "        while cap.isOpened():\n",
    "            ret, _ = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            count += 1\n",
    "        cap.release()\n",
    "        frame_counts.append(count)\n",
    "\n",
    "    return frame_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_frame_distribution(frame_counts):\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    sns.violinplot(x=frame_counts)\n",
    "    plt.title(\"Violin Plot of Frame Counts per Video\")\n",
    "    plt.xlabel(\"Number of Frames\")\n",
    "    plt.xlabel(\"Number of Frames\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_counts = count_frames_per_video(train_df[\"video_path\"].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standard deviation of the frame counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.std(frame_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_frame_distribution(frame_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Video preprocessing utilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Function to resize the video frames to a square shape without distorting their content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-06T02:12:09.486337Z",
     "iopub.status.busy": "2024-11-06T02:12:09.486016Z",
     "iopub.status.idle": "2024-11-06T02:12:09.496479Z",
     "shell.execute_reply": "2024-11-06T02:12:09.495613Z",
     "shell.execute_reply.started": "2024-11-06T02:12:09.486305Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def crop_center_square(frame):\n",
    "    y, x = frame.shape[0:2]  # Get the height (y) and width (x) of the image\n",
    "    min_dim = min(y, x)       # Find the smallest dimension (either height or width)\n",
    "    start_x = (x // 2) - (min_dim // 2)  # Calculate the horizontal starting point for the crop\n",
    "    start_y = (y // 2) - (min_dim // 2)  # Calculate the vertical starting point for the crop\n",
    "    \n",
    "    return frame[start_y : start_y + min_dim, start_x : start_x + min_dim]  # Return the cropped square"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Crop,resize, and reorder color channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_preprocess_video(video_path, max_frames=FRAME_COUNT, resize=(FRAME_SIZE, FRAME_SIZE)):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    frames = []\n",
    "    \n",
    "    try:\n",
    "        while True:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            frame = crop_center_square(frame)\n",
    "            frame = cv2.resize(frame, resize)\n",
    "            frame = frame[:, :, [2, 1, 0]] # Reorder the color channels from OpenCV BGR to RGB\n",
    "            \n",
    "            # Normalize pixel values to the range [0, 1] and convert to float32 for TensorFlow\n",
    "            frame = frame.astype(np.float32) / 255.0\n",
    "            \n",
    "            frames.append(frame)\n",
    "\n",
    "            if len(frames) == max_frames:\n",
    "                break\n",
    "    finally:\n",
    "        cap.release()\n",
    "\n",
    "    frames = np.array(frames)\n",
    "    \n",
    "    if frames.shape[0] < max_frames:\n",
    "        # Create zero padding for frames with shape (max_frames - current_frame_count, height, width, channels)\n",
    "        padding = np.zeros((max_frames - frames.shape[0], *frames.shape[1:]), dtype=np.float32)\n",
    "        frames = np.concatenate([frames, padding], axis=0)\n",
    "\n",
    "    frames_tensor = tf.convert_to_tensor(frames)\n",
    "\n",
    "    # Create mask to indicate valid frames (1 for valid, 0 for padding)\n",
    "    mask = tf.cast(tf.sequence_mask([frames.shape[0]], max_frames), tf.float32)\n",
    "\n",
    "    return frames_tensor, mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encode the class labels as integers using the Keras StringLookup layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_processor = keras.layers.StringLookup(num_oov_indices=0, vocabulary=sports_actions)\n",
    "\n",
    "class_vocab = label_processor.get_vocabulary()\n",
    "\n",
    "print(f\"Vocabulary: {class_vocab}\")\n",
    "print(f\"Number of classes: {len(class_vocab)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load all the videos from a dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_videos(dataset, dataset_type):\n",
    "    start_time = time.time()\n",
    "\n",
    "    video_frames = []\n",
    "    masks = []\n",
    "    labels = keras.ops.convert_to_numpy(label_processor(dataset[\"label\"].values[..., None]))\n",
    "\n",
    "    for _, row in dataset.iterrows():\n",
    "        video_path = row[\"video_path\"]\n",
    "\n",
    "        frames, mask = load_and_preprocess_video(\n",
    "            video_path, max_frames=FRAME_COUNT, resize=(FRAME_SIZE, FRAME_SIZE)\n",
    "        )\n",
    "\n",
    "        video_frames.append(frames)\n",
    "        masks.append(mask)\n",
    "\n",
    "    # Convert lists to NumPy arrays first to ensure consistency in shape\n",
    "    video_frames = np.array(video_frames)\n",
    "    labels = np.array(labels)\n",
    "    masks = np.array(masks)\n",
    "\n",
    "    # Convert to TensorFlow tensors\n",
    "    video_frames = tf.convert_to_tensor(video_frames, dtype=tf.float32)\n",
    "    labels = tf.convert_to_tensor(labels, dtype=tf.float32)\n",
    "    masks = tf.convert_to_tensor(masks, dtype=tf.float32)\n",
    "\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    time_unit = \"seconds\" if elapsed_time < 60 else \"minutes\"\n",
    "    time_value = elapsed_time if elapsed_time < 60 else elapsed_time / 60\n",
    "    print(f\"Processed {len(video_frames)} {dataset_type} videos in {time_value:.2f} {time_unit}\")\n",
    "\n",
    "    return video_frames, masks, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading the preprocessed video frames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract the frame features, feature masks and labels for the `train` set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-06T02:12:17.636499Z",
     "iopub.status.busy": "2024-11-06T02:12:17.636194Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train_data, train_masks, train_labels = load_videos(train_df, \"Train\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract the frame features, feature masks and labels for the `validation` set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_data, val_masks, val_labels = load_videos(val_df, \"Validation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract the frame features, feature masks and labels for the `test` set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data, test_masks, test_labels = load_videos(test_df, \"Test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_masks.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "total_samples, frame_count, _, _, _ = train_data.shape\n",
    "\n",
    "print(f\"\"\"Video frames in train set: {train_data.shape} \n",
    "    → {total_samples} samples\n",
    "    → {frame_count} frames per video\n",
    "    → {FRAME_SIZE}x{FRAME_SIZE} resolution\n",
    "    → {3} channels\n",
    "\"\"\")\n",
    "\n",
    "total_samples, _, mask_count = train_masks.shape\n",
    "print(f\"\"\"Frame masks in train set: {train_data.shape} \n",
    "    → {total_samples} samples\n",
    "    → {mask_count} masks per video\n",
    "\"\"\")\n",
    "\n",
    "print(f\"Video frames in validation set: {val_data.shape}\")\n",
    "print(f\"Frame masks in validation set: {val_data.shape}\")\n",
    "\n",
    "print(f\"\\nVideo frames in test set: {test_data.shape}\")\n",
    "print(f\"Frame masks in test set: {test_data.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preparing the dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataloader(\n",
    "    videos: tf.Tensor,\n",
    "    labels: tf.Tensor,\n",
    "    masks: tf.Tensor,\n",
    "    loader_type: str = \"train\",\n",
    "):\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((videos, labels, masks))\n",
    "\n",
    "    if loader_type == \"train\":\n",
    "        dataset = dataset.shuffle(BATCH_SIZE * 2)\n",
    "\n",
    "    dataloader = dataset.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = prepare_dataloader(train_data, train_labels, train_masks, loader_type=\"train\")\n",
    "val_loader = prepare_dataloader(val_data, val_labels, val_masks, loader_type=\"val\")\n",
    "test_dataloader = prepare_dataloader(test_data, test_labels, test_masks, loader_type=\"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Video Vision Transformer(ViVit) Model Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tubelet Embedding\n",
    "- Capturing Temporal Information by Extracting and Flattening Video Volumes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TubeletEmbedding(layers.Layer):\n",
    "    def __init__(self, embed_dim, patch_size, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.projection = layers.Conv3D(\n",
    "            filters=embed_dim,\n",
    "            kernel_size=patch_size,\n",
    "            strides=patch_size,\n",
    "            padding=\"VALID\",\n",
    "        )\n",
    "        self.flatten = layers.Reshape(target_shape=(-1, embed_dim))\n",
    "\n",
    "    def call(self, videos):\n",
    "        projected_patches = self.projection(videos)\n",
    "        flattened_patches = self.flatten(projected_patches)\n",
    "        return flattened_patches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Positional Embedding\n",
    "- This layer adds positional information to the encoded video tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoder(layers.Layer):\n",
    "    def __init__(self, embed_dim, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.embed_dim = embed_dim\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        _, num_tokens, _ = input_shape\n",
    "        self.position_embedding = layers.Embedding(\n",
    "            input_dim=num_tokens, output_dim=self.embed_dim\n",
    "        )\n",
    "        self.positions = ops.arange(0, num_tokens, 1)\n",
    "\n",
    "    def call(self, encoded_tokens):\n",
    "        # Encode the positions and add it to the encoded tokens\n",
    "        encoded_positions = self.position_embedding(self.positions)\n",
    "        encoded_tokens = encoded_tokens + encoded_positions\n",
    "        return encoded_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Spatio-temporal attention model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vivit_classifier(tubelet_embedder, positional_encoder):\n",
    "    inputs = layers.Input(shape=INPUT_SHAPE)\n",
    "    patches = tubelet_embedder(inputs)\n",
    "    encoded_patches = positional_encoder(patches)\n",
    "\n",
    "    # Create multiple layers of the Transformer block.\n",
    "    for _ in range(NUM_LAYERS):\n",
    "        # Layer normalization and MHSA\n",
    "        x1 = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
    "        attention_output = layers.MultiHeadAttention(\n",
    "            num_heads=NUM_HEADS, key_dim=PROJECTION_DIM // NUM_HEADS, dropout=0.1\n",
    "        )(x1, x1)\n",
    "\n",
    "        # Skip connection\n",
    "        x2 = layers.Add()([attention_output, encoded_patches])\n",
    "\n",
    "        # Layer Normalization and MLP\n",
    "        x3 = layers.LayerNormalization(epsilon=1e-6)(x2)\n",
    "        x3 = keras.Sequential(\n",
    "            [\n",
    "                layers.Dense(units=PROJECTION_DIM * 4, activation=ops.gelu),\n",
    "                layers.Dense(units=PROJECTION_DIM, activation=ops.gelu),\n",
    "            ]\n",
    "        )(x3)\n",
    "\n",
    "        # Skip connection\n",
    "        encoded_patches = layers.Add()([x3, x2])\n",
    "\n",
    "    # Layer normalization and Global average pooling.\n",
    "    representation = layers.LayerNormalization(epsilon=LAYER_NORM_EPS)(encoded_patches)\n",
    "    representation = layers.GlobalAvgPool1D()(representation)\n",
    "\n",
    "    # Classify outputs\n",
    "    outputs = layers.Dense(units=NUM_CLASSES, activation=\"softmax\")(representation)\n",
    "\n",
    "    # Create the Keras model\n",
    "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model compilation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compile_transformer_model(show_summary=True):\n",
    "    tubelet_embedder = TubeletEmbedding(embed_dim=PROJECTION_DIM, patch_size=PATCH_SIZE)\n",
    "    positional_encoder = PositionalEncoder(embed_dim=PROJECTION_DIM)\n",
    "\n",
    "    model = create_vivit_classifier(tubelet_embedder, positional_encoder)\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(learning_rate=LEARNING_RATE),\n",
    "        loss=\"sparse_categorical_crossentropy\",\n",
    "        metrics=[\n",
    "            keras.metrics.SparseCategoricalAccuracy(name=\"accuracy\"),\n",
    "            keras.metrics.SparseTopKCategoricalAccuracy(5, name=\"top-5-accuracy\"),\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    if show_summary:\n",
    "        model.summary()\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer_model = compile_transformer_model(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, epochs=EPOCHS):\n",
    "    checkpoint = ModelCheckpoint(\n",
    "        MODEL_PATH,\n",
    "        monitor=\"val_loss\",\n",
    "        mode=\"min\",  # Save the model when the loss decreases (when model improves)\n",
    "        save_weights_only=False,\n",
    "        save_best_only=True,\n",
    "        verbose=1,\n",
    "    )\n",
    "\n",
    "    # TODO: Experiment with this\n",
    "    early_stopping = EarlyStopping(\n",
    "        monitor=\"val_loss\", patience=5, restore_best_weights=True, verbose=1\n",
    "    )\n",
    "\n",
    "    history = model.fit(\n",
    "        train_loader,\n",
    "        epochs=epochs,\n",
    "        validation_data=val_loader,\n",
    "        callbacks=[checkpoint, early_stopping],\n",
    "    )\n",
    "\n",
    "    return history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = train_model(transformer_model, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(MODEL_PATH):\n",
    "    print(f\"Model saved at: {MODEL_PATH}\")\n",
    "    model_size = os.path.getsize(MODEL_PATH) / (1024 * 1024)\n",
    "    print(f\"Model size: {model_size:.2f} MB\")\n",
    "else:\n",
    "    print(\"Model file not found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualise the training and validation loss and accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_training_metrics(history):\n",
    "    train_loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "    train_accuracy = history.history['accuracy']\n",
    "    val_accuracy = history.history['val_accuracy']\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    # Loss plot\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(train_loss, label='Training Loss')\n",
    "    plt.plot(val_loss, label='Validation Loss')\n",
    "    plt.title(\"Training and Validation Loss\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()\n",
    "\n",
    "    # Accuracy plot\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(train_accuracy, label='Training Accuracy')\n",
    "    plt.plot(val_accuracy, label='Validation Accuracy')\n",
    "    plt.title(\"Training and Validation Accuracy\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_training_metrics(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Testing & Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate model on the test data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, test_loader):\n",
    "    print(\"\\nEvaluating the model on the test set...\")\n",
    "    model.load_weights(MODEL_PATH)\n",
    "\n",
    "    _, accuracy, top_5_accuracy = model.evaluate(test_loader)\n",
    "    print(f\"Test accuracy: {round(accuracy * 100, 2)}%\")\n",
    "    print(f\"Test top 5 accuracy: {round(top_5_accuracy * 100, 2)}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, test_loader): \n",
    "    model.load_weights(MODEL_PATH)\n",
    "\n",
    "    _, accuracy, top_5_accuracy = model.evaluate(test_loader)\n",
    "    print(f\"Test accuracy: {round(accuracy * 100, 2)}%\")\n",
    "    print(f\"Test top 5 accuracy: {round(top_5_accuracy * 100, 2)}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model(transformer_model, test_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation with single video"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make predictions on a single video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sequence_prediction(model, vide_frames, video_mask, true_label):\n",
    "    probabilities = model.predict([vide_frames, video_mask])[0]\n",
    "        \n",
    "    print(\"\\nTop-5 actions:\")\n",
    "    for i in np.argsort(probabilities)[::-1][:5]:\n",
    "        print(f\"  - {class_vocab[i]}: {probabilities[i] * 100:5.2f}%\")\n",
    "    \n",
    "    # Get the top-1 predicted label (highest probability)\n",
    "    predicted_index = np.argmax(probabilities)\n",
    "    predicted_label = class_vocab[predicted_index]\n",
    "\n",
    "    # Convert true_label to index to align with predicted index format\n",
    "    true_label_index = class_vocab.index(true_label)\n",
    "\n",
    "    # Prepare y_true and y_pred as binary arrays (1 for correct label, 0 for others)\n",
    "    y_true = np.zeros(len(class_vocab))\n",
    "    y_pred = np.zeros(len(class_vocab))\n",
    "    y_true[true_label_index] = 1\n",
    "    y_pred[predicted_index] = 1\n",
    " \n",
    "    return y_true, y_pred, predicted_label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display predicted image as GIF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    }
   },
   "outputs": [],
   "source": [
    "def display_as_gif(images):\n",
    "    converted_images = images.astype(np.uint8)\n",
    "    imageio.mimsave(\"animation.gif\", converted_images, duration=100)\n",
    "    return Image(\"animation.gif\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choosing a random video to use for prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_index = np.random.randint(len(test_data))\n",
    "\n",
    "# Get the test video path\n",
    "test_video_path = test_data[\"video_path\"].values[random_index]\n",
    "\n",
    "# Get the true label of the test video\n",
    "true_label_index = test_labels.tolist()[random_index][0]\n",
    "true_label = class_vocab[true_label_index]\n",
    "\n",
    "test_video_frames, test_video_mask = load_and_preprocess_video(test_video_path)\n",
    "\n",
    "print(f\"Test video path: {test_video_path}\")\n",
    "print(f\"Label: {true_label}\")\n",
    "\n",
    "print(f\"\"\"\n",
    "Test video frames shape:\n",
    "  - {test_video_frames.shape[0]} frames\n",
    "  - {test_video_frames.shape[1]} pixels (height) x {test_video_frames.shape[2]} pixels (width)\n",
    "  - {test_video_frames.shape[3]} color channels\n",
    "\"\"\")\n",
    "\n",
    "display_as_gif(test_video_frames[:FRAME_COUNT])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the prediction on the test video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true, y_pred, predicted_label = sequence_prediction(\n",
    "    transformer_model, test_video_frames, test_video_mask, true_label\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\nTrue label: {true_label}\")\n",
    "print(f\"Predicted label: {predicted_label}\")\n",
    "\n",
    "print(f\"y_true: {y_true}\")\n",
    "print(f\"y_pred: {y_pred}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation on the entire test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_evaluation_metrics(y_true, y_pred, predictions, target_classes):\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    print(f\"Top-1 Accuracy: {accuracy * 100:.2f}%\")\n",
    "\n",
    "    k = 2\n",
    "    top_k_acc = top_k_accuracy_score(y_true, predictions, k=k)\n",
    "    print(f\"Top-{k} Accuracy: {top_k_acc * 100:.2f}%\")\n",
    "\n",
    "    report = classification_report(y_true, y_pred, target_names=target_classes, zero_division=0)\n",
    "    print(\"\\nClassification Report:\\n\", report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = transformer_model.predict([test_data, test_masks], batch_size=BATCH_SIZE)\n",
    "predicted_classes = np.argmax(predictions, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_evaluation_metrics(test_labels, predicted_classes, predictions, class_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_confusion_matrix(y_true, y_pred, target_classes, show_plot=False):\n",
    "    conf_matrix = confusion_matrix(y_true, y_pred)\n",
    "    \n",
    "    if show_plot:\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        sns.heatmap(conf_matrix, fmt=\"d\", xticklabels=target_classes, yticklabels=target_classes)\n",
    "        plt.xlabel(\"Predicted\")\n",
    "        plt.ylabel(\"True\")\n",
    "        plt.title(\"Confusion Matrix\")\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(conf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_labels = np.array(test_labels).flatten()\n",
    "display_confusion_matrix(true_labels, predicted_classes, class_vocab, show_plot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Minimal Implementations, \"Hyperparameter Tuning Analysis,\" WandB. [Online]. Available: https://wandb.ai/minimal-implementations/vivit/reports/Hyperparameter-Tuning-Analysis--VmlldzoxNDEwNzcx"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 4915922,
     "datasetId": 2807884,
     "sourceId": 4849320,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30787,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
