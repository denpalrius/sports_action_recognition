{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sports Action Recognition Using CNN-RNN Architecture with Transfer Learning on `UCF101 10 Sports actions`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-06T02:10:35.139379Z",
     "iopub.status.busy": "2024-11-06T02:10:35.138805Z",
     "iopub.status.idle": "2024-11-06T02:10:51.356809Z",
     "shell.execute_reply": "2024-11-06T02:10:51.355557Z",
     "shell.execute_reply.started": "2024-11-06T02:10:35.139326Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install kagglehub wandb -Uq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-11-06T02:10:51.359159Z",
     "iopub.status.busy": "2024-11-06T02:10:51.358754Z",
     "iopub.status.idle": "2024-11-06T02:11:09.254053Z",
     "shell.execute_reply": "2024-11-06T02:11:09.253134Z",
     "shell.execute_reply.started": "2024-11-06T02:10:51.359087Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import accuracy_score, top_k_accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "import imageio\n",
    "import cv2\n",
    "from IPython.display import Image\n",
    "\n",
    "import keras\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from keras.metrics import top_k_categorical_accuracy\n",
    "\n",
    "import pprint\n",
    "\n",
    "import kagglehub\n",
    "\n",
    "import wandb\n",
    "from wandb.integration.keras import WandbCallback"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download latest version of the ucf101-action-recognition dataset from Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = kagglehub.dataset_download(\"matthewjansen/ucf101-action-recognition\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-06T02:12:09.427435Z",
     "iopub.status.busy": "2024-11-06T02:12:09.427055Z",
     "iopub.status.idle": "2024-11-06T02:12:09.431825Z",
     "shell.execute_reply": "2024-11-06T02:12:09.430893Z",
     "shell.execute_reply.started": "2024-11-06T02:12:09.427384Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(\"Path to dataset files: \\n\", path)\n",
    "print(\"\\nFiles in dataset directory:\\n\", os.listdir(path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Class Definition**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sports_actions = [\n",
    "    \"SkyDiving\",\n",
    "    \"Biking\",\n",
    "    \"HorseRace\",\n",
    "    \"Surfing\",\n",
    "    \"TennisSwing\",\n",
    "    \"Punch\",\n",
    "    \"Basketball\",\n",
    "    \"JumpRope\",\n",
    "    \"Archery\",\n",
    "    \"Skiing\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utility to transform video paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(dataset_type):\n",
    "    dataset_path = os.path.join(path, f\"{dataset_type}.csv\")\n",
    "    dataset = pd.read_csv(dataset_path)\n",
    "\n",
    "    # Filter dataset to only include the specified sports actions\n",
    "    filtered_dataset = dataset[dataset[\"label\"].isin(sports_actions)]\n",
    "\n",
    "    return pd.DataFrame(\n",
    "        {\n",
    "            \"label\": filtered_dataset[\"label\"],\n",
    "            \"video_name\": filtered_dataset[\"clip_name\"],\n",
    "            \"rel_path\": filtered_dataset[\"clip_path\"],\n",
    "            \"video_path\": filtered_dataset[\"clip_path\"].apply(lambda x: f\"{path}{x}\"),\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the train and test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = load_dataset(\"train\")\n",
    "val_df = load_dataset(\"val\")\n",
    "test_df = load_dataset(\"test\")\n",
    "\n",
    "print(f\"Total videos for training: {len(train_df)}\")\n",
    "print(f\"Total videos for validation: {len(val_df)}\")\n",
    "print(f\"Total videos for testing: {len(test_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of unique classes in training set: \", len(train_df[\"label\"].unique()))\n",
    "print(\"Number of unique classes in validation set: \", len(val_df[\"label\"].unique()))\n",
    "print(\"Number of unique classes in test set: \", len(test_df[\"label\"].unique()))\n",
    "\n",
    "print(\"\\nLabels: \\n\", train_df[\"label\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configs & Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-06T02:12:09.480471Z",
     "iopub.status.busy": "2024-11-06T02:12:09.480187Z",
     "iopub.status.idle": "2024-11-06T02:12:09.484927Z",
     "shell.execute_reply": "2024-11-06T02:12:09.484040Z",
     "shell.execute_reply.started": "2024-11-06T02:12:09.480440Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "MODEL_NAME = \"cnn_rnn_ucf101_10c_tl\"\n",
    "MODEL_ROOT_PATH = \"../models\"\n",
    "\n",
    "IMG_SIZE = 224\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 100\n",
    "LEARNING_RATE = 1e-4\n",
    "\n",
    "MAX_SEQ_LENGTH = 20 # TODO: Use 150 for final test\n",
    "NUM_FEATURES = 2048\n",
    "\n",
    "HYPERPARAMETER_TUNING_ENABLED = True\n",
    "TUNING_EPISODES = 40"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Video Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Review video category distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def review_class_distribution(dataset, dataset_name):\n",
    "    class_counts = dataset[\"label\"].value_counts()\n",
    "    return class_counts\n",
    "\n",
    "# Get class distributions for each dataset\n",
    "train_class_counts = review_class_distribution(train_df, \"Train\")\n",
    "val_class_counts = review_class_distribution(val_df, \"Validation\")\n",
    "test_class_counts = review_class_distribution(test_df, \"Test\")\n",
    "\n",
    "# Create DataFrame for distribution and calculate average\n",
    "distribution_df = pd.DataFrame({\n",
    "    \"Train\": train_class_counts,\n",
    "    \"Validation\": val_class_counts,\n",
    "    \"Test\": test_class_counts\n",
    "}).fillna(0)\n",
    "\n",
    "distribution_df[\"Average\"] = distribution_df.mean(axis=1).round().astype(int)\n",
    "print(\"Combined average number of videos per class:\")\n",
    "print(distribution_df)\n",
    "\n",
    "# Plot the distribution\n",
    "plot_distribution_df = distribution_df.drop(columns=\"Average\")\n",
    "plot_distribution_df.plot(kind=\"bar\", figsize=(10, 5))\n",
    "plt.title(\"Class Distribution Comparison Across Train, Validation, and Test Sets\")\n",
    "plt.xlabel(\"Class Labels\")\n",
    "plt.ylabel(\"Number of Videos\")\n",
    "plt.legend(title=\"Dataset\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Review Video frame distribution "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Count the number of frames for each video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_frames_per_video(video_paths):\n",
    "    frame_counts = []\n",
    "\n",
    "    for video_path in video_paths: \n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        count = 0\n",
    "        \n",
    "        while cap.isOpened():\n",
    "            ret, _ = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            count += 1\n",
    "        cap.release()\n",
    "        frame_counts.append(count)\n",
    "\n",
    "    return frame_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_frame_distribution(frame_counts):\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    sns.violinplot(x=frame_counts)\n",
    "    plt.title(\"Violin Plot of Frame Counts per Video\")\n",
    "    plt.xlabel(\"Number of Frames\")\n",
    "    plt.xlabel(\"Number of Frames\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_counts = count_frames_per_video(train_df[\"video_path\"].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standard deviation of the frame counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.std(frame_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_frame_distribution(frame_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Video preprocessing utilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Function to resize the video frames to a square shape without distorting their content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-06T02:12:09.486337Z",
     "iopub.status.busy": "2024-11-06T02:12:09.486016Z",
     "iopub.status.idle": "2024-11-06T02:12:09.496479Z",
     "shell.execute_reply": "2024-11-06T02:12:09.495613Z",
     "shell.execute_reply.started": "2024-11-06T02:12:09.486305Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def crop_center_square(frame):\n",
    "    y, x = frame.shape[0:2]  # Get the height (y) and width (x) of the image\n",
    "    min_dim = min(y, x)       # Find the smallest dimension (either height or width)\n",
    "    start_x = (x // 2) - (min_dim // 2)  # Calculate the horizontal starting point for the crop\n",
    "    start_y = (y // 2) - (min_dim // 2)  # Calculate the vertical starting point for the crop\n",
    "    \n",
    "    return frame[start_y : start_y + min_dim, start_x : start_x + min_dim]  # Return the cropped square"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Crop,resize, and reorder color channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-06T02:12:09.499763Z",
     "iopub.status.busy": "2024-11-06T02:12:09.499469Z",
     "iopub.status.idle": "2024-11-06T02:12:09.507023Z",
     "shell.execute_reply": "2024-11-06T02:12:09.506339Z",
     "shell.execute_reply.started": "2024-11-06T02:12:09.499731Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def load_and_preprocess_video(video_path, max_frames=0, resize=(IMG_SIZE, IMG_SIZE)):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    frames = []\n",
    "    try:\n",
    "        while True:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            frame = crop_center_square(frame) # Crop center square\n",
    "            frame = cv2.resize(frame, resize) # Resize the image (In this case to 224x224)\n",
    "            frame = frame[:, :, [2, 1, 0]] # Reorder the color channels from OpenCV BGR to RGB\n",
    "            frames.append(frame)\n",
    "\n",
    "            if len(frames) == max_frames:\n",
    "                break\n",
    "    finally:\n",
    "        cap.release()\n",
    "   \n",
    "    return np.array(frames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using a pre-trained network to extract meaningful features from the extracted frames, the InceptionV3 model pretrained on ImageNet-1k dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-06T02:12:09.510210Z",
     "iopub.status.busy": "2024-11-06T02:12:09.507987Z",
     "iopub.status.idle": "2024-11-06T02:12:17.591303Z",
     "shell.execute_reply": "2024-11-06T02:12:17.590307Z",
     "shell.execute_reply.started": "2024-11-06T02:12:09.510168Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def build_feature_extractor():\n",
    "    feature_extractor = keras.applications.InceptionV3(\n",
    "        weights=\"imagenet\",\n",
    "        include_top=False,\n",
    "        pooling=\"avg\",\n",
    "        input_shape=(IMG_SIZE, IMG_SIZE, 3),\n",
    "    )\n",
    "    preprocess_input = keras.applications.inception_v3.preprocess_input\n",
    "\n",
    "    inputs = keras.Input((IMG_SIZE, IMG_SIZE, 3))\n",
    "    preprocessed = preprocess_input(inputs)\n",
    "\n",
    "    outputs = feature_extractor(preprocessed)\n",
    "    return keras.Model(inputs, outputs, name=\"feature_extractor\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "feature_extractor = build_feature_extractor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encode the class labels as integers using the Keras StringLookup layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(train_df[\"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-06T02:12:17.592738Z",
     "iopub.status.busy": "2024-11-06T02:12:17.592428Z",
     "iopub.status.idle": "2024-11-06T02:12:17.622574Z",
     "shell.execute_reply": "2024-11-06T02:12:17.621683Z",
     "shell.execute_reply.started": "2024-11-06T02:12:17.592705Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "label_processor = keras.layers.StringLookup(num_oov_indices=0, vocabulary=sports_actions)\n",
    "\n",
    "class_vocab = label_processor.get_vocabulary()\n",
    "\n",
    "print(f\"Vocabulary: {class_vocab}\")\n",
    "print(f\"Number of classes: {len(class_vocab)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "label_processorExtract frame_features, frame_masks and labels\n",
    "- `frame_features` will contain extracted features per frame\n",
    "- `frame_masks` will contain booleans denoting if a timestep/frame is padded or not"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper function to create masks and features for a single video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_video_mask_and_features(frames):\n",
    "    frames = frames[None, ...]  # Add batch dimension\n",
    "    video_length = min(MAX_SEQ_LENGTH, frames.shape[1])\n",
    "\n",
    "    mask = np.zeros((1, MAX_SEQ_LENGTH), dtype=\"bool\")\n",
    "    features = np.zeros((1, MAX_SEQ_LENGTH, NUM_FEATURES), dtype=\"float32\")\n",
    "\n",
    "    # TODO: Check effect of normalisation and srandardisation\n",
    "    \n",
    "    for j in range(video_length):\n",
    "        features[0, j, :] = feature_extractor.predict(frames[:, j, :], verbose=0)\n",
    "    mask[0, :video_length] = 1  # Set mask for valid frames\n",
    "\n",
    "    return features, mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main function for video feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features_and_masks(df, dataset_type):\n",
    "    start_time = time.time()\n",
    "    num_samples = len(df)\n",
    "    video_paths = df[\"video_path\"].values.tolist()\n",
    "    labels = keras.ops.convert_to_numpy(label_processor(df[\"label\"].values[..., None]))\n",
    "\n",
    "    frame_masks = np.zeros((num_samples, MAX_SEQ_LENGTH), dtype=\"bool\")\n",
    "    frame_features = np.zeros((num_samples, MAX_SEQ_LENGTH, NUM_FEATURES), dtype=\"float32\")\n",
    "\n",
    "    for idx, video_path in enumerate(video_paths):\n",
    "        frames = load_and_preprocess_video(video_path)\n",
    "        features, mask = create_video_mask_and_features(frames)\n",
    "        frame_features[idx] = features\n",
    "        frame_masks[idx] = mask\n",
    "\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    time_unit = \"seconds\" if elapsed_time < 60 else \"minutes\"\n",
    "    time_value = elapsed_time if elapsed_time < 60 else elapsed_time / 60\n",
    "    print(f\"Processed {num_samples} {dataset_type} videos in {time_value:.2f} {time_unit}\")\n",
    "\n",
    "    return (frame_features, frame_masks), labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract the frame features, feature masks and labels for the `train` set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-06T02:12:17.636499Z",
     "iopub.status.busy": "2024-11-06T02:12:17.636194Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train_data, train_labels = extract_features_and_masks(train_df, \"Train\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract the frame features, feature masks and labels for the `validation` set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_data, val_labels = extract_features_and_masks(val_df, \"Validation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract the frame features, feature masks and labels for the `test` set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data, test_labels = extract_features_and_masks(test_df, \"Test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "total_samples, frame_count, num_of_features = train_data[0].shape\n",
    "\n",
    "print(f\"\"\"Frame features in train set: {train_data[0].shape} \n",
    "    → {total_samples} samples\n",
    "    → {frame_count} frames per video\n",
    "    → {num_of_features} features per frame\n",
    "\"\"\")\n",
    "\n",
    "total_samples, mask_count = train_data[1].shape\n",
    "print(f\"\"\"Frame masks in train set: {train_data[1].shape} \n",
    "    → {total_samples} samples\n",
    "    → {mask_count} masks per video\n",
    "\"\"\")\n",
    "\n",
    "print(f\"Frame features in validation set: {val_data[0].shape}\")\n",
    "print(f\"Frame masks in validation set: {val_data[1].shape}\")\n",
    "\n",
    "print(f\"Frame features in test set: {test_data[0].shape}\")\n",
    "print(f\"Frame masks in test set: {test_data[1].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weights & Biases Sweep Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if HYPERPARAMETER_TUNING_ENABLED:\n",
    "    wandb.login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Weights & Biases Sweep Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sweep_config = {\n",
    "    \"method\": \"bayes\",  # Efficient hyperparameter search using Bayesian Optimization\n",
    "    \"metric\": {\"name\": \"val_accuracy\", \"goal\": \"maximize\"},\n",
    "    \"parameters\": {\n",
    "        # Search space for learning rate\n",
    "        \"learning_rate\": {\"values\": [1e-4, 1e-3, 1e-2]},\n",
    "        # Batch sizes to test\n",
    "        \"batch_size\": {\"values\": [32, 64, 128]},\n",
    "        # Number of units in GRU layers\n",
    "        \"gru_units\": {\"values\": [8, 12, 16]},\n",
    "        # Dropout rates for layers\n",
    "        \"dropout_rate\": {\"values\": [0.3, 0.4, 0.5]},\n",
    "        # Different optimizers to test\n",
    "        \"optimizer\": {\"values\": [\"adam\", \"sgd\", \"rmsprop\"]},\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initializing the Sweep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if HYPERPARAMETER_TUNING_ENABLED:\n",
    "    sweep_id = wandb.sweep(sweep_config, project=MODEL_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN GRU Model Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recurrent Neural Network architecture with GRU (Gated Recurrent Unit) layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_rnn_sequence_model(config):\n",
    "    frame_features_input = keras.Input((MAX_SEQ_LENGTH, NUM_FEATURES))\n",
    "    mask_input = keras.Input((MAX_SEQ_LENGTH,), dtype=\"bool\")\n",
    "\n",
    "    x = keras.layers.Bidirectional(\n",
    "        keras.layers.GRU(config.get(\"gru_units\", 12),\n",
    "                         return_sequences=True,\n",
    "                         recurrent_dropout=config.get(\"dropout_rate\", 0.3),\n",
    "                         kernel_regularizer=keras.regularizers.L1(0.01),\n",
    "                         activity_regularizer=keras.regularizers.L2(0.01))\n",
    "    )(frame_features_input, mask=mask_input)\n",
    "\n",
    "    x = keras.layers.GRU(config.get(\"gru_units\", 8),\n",
    "                         recurrent_dropout=config.get(\"dropout_rate\", 0.4),\n",
    "                         kernel_regularizer=keras.regularizers.L1(0.01),\n",
    "                         activity_regularizer=keras.regularizers.L2(0.01))(x)\n",
    "    x = keras.layers.Dropout(config.get(\"dropout_rate\", 0.5))(x)\n",
    "\n",
    "    x = keras.layers.Dense(32, activation=\"relu\",\n",
    "                           kernel_regularizer=keras.regularizers.L1(0.01),\n",
    "                           activity_regularizer=keras.regularizers.L2(0.01))(x)\n",
    "    x = keras.layers.BatchNormalization()(x)\n",
    "    x = keras.layers.Dropout(config.get(\"dropout_rate\", 0.4))(x)\n",
    "\n",
    "    x = keras.layers.Dense(64, activation=\"relu\",\n",
    "                           kernel_regularizer=keras.regularizers.L1(0.01),\n",
    "                           activity_regularizer=keras.regularizers.L2(0.01))(x)\n",
    "    x = keras.layers.BatchNormalization()(x)\n",
    "    x = keras.layers.Dropout(config.get(\"dropout_rate\", 0.4))(x)\n",
    "\n",
    "    output = keras.layers.Dense(len(class_vocab), activation=\"softmax\")(x)\n",
    "\n",
    "    rnn_model = keras.Model([frame_features_input, mask_input], output)\n",
    "\n",
    "    # Configure optimizer\n",
    "    optimizer = config.get(\"optimizer\", \"adam\")\n",
    "    learning_rate = config.get(\"learning_rate\", 1e-3)\n",
    "    if optimizer == \"adam\":\n",
    "        optimizer = keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "    elif optimizer == \"sgd\":\n",
    "        optimizer = keras.optimizers.SGD(learning_rate=learning_rate)\n",
    "    elif optimizer == \"rmsprop\":\n",
    "        optimizer = keras.optimizers.RMSprop(learning_rate=learning_rate)\n",
    "\n",
    "    rnn_model.compile(\n",
    "        loss=\"sparse_categorical_crossentropy\",\n",
    "        optimizer=optimizer,\n",
    "        metrics=[\"accuracy\"],\n",
    "    )\n",
    "\n",
    "    return rnn_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_path():\n",
    "    model_version = f'v{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}'\n",
    "    model_base_path = os.path.join(MODEL_ROOT_PATH, model_version)\n",
    "    model_path = os.path.join(MODEL_ROOT_PATH, model_version, f'{MODEL_NAME}.keras')\n",
    "\n",
    "    print(f\"Model will be saved to: '{model_path}'\")\n",
    "\n",
    "    return model_version, model_base_path, model_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_version, model_base_path, model_path = get_model_path()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    if HYPERPARAMETER_TUNING_ENABLED:\n",
    "        wandb.init()\n",
    "        config = wandb.config\n",
    "    else:\n",
    "        config = {\n",
    "            \"learning_rate\": 1e-3,\n",
    "            \"batch_size\": 32,\n",
    "            \"gru_units\": 12,\n",
    "            \"dropout_rate\": 0.4,\n",
    "            \"optimizer\": \"adam\",\n",
    "        }\n",
    "\n",
    "    model = build_rnn_sequence_model(config)\n",
    "    return model, config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_seq_model, config = build_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_seq_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utility to run the experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def train_and_evaluate_model(config=None):\n",
    "    checkpoint = ModelCheckpoint(\n",
    "        model_path,\n",
    "        monitor=\"val_loss\",\n",
    "        mode=\"min\",  # Save the model when the loss decreases (when model improves)\n",
    "        save_weights_only=False,\n",
    "        save_best_only=True,\n",
    "        verbose=1,\n",
    "    )\n",
    "\n",
    "    # TODO: Tune this\n",
    "    early_stopping = EarlyStopping(\n",
    "        monitor=\"val_loss\",\n",
    "        patience=5,\n",
    "        min_delta=0.001,\n",
    "        restore_best_weights=True,\n",
    "        verbose=1,\n",
    "    )\n",
    "    \n",
    "    callbacks = [checkpoint, early_stopping]\n",
    "    if HYPERPARAMETER_TUNING_ENABLED:\n",
    "        callbacks.append(WandbCallback())\n",
    "\n",
    "    \n",
    "    # Train the model\n",
    "    history = rnn_seq_model.fit(\n",
    "        [train_data[0], train_data[1]],\n",
    "        train_labels,\n",
    "        validation_data=([val_data[0], val_data[1]], val_labels),\n",
    "        batch_size=config.get(\"batch_size\", 64),\n",
    "        epochs=EPOCHS,\n",
    "        callbacks=callbacks,\n",
    "    )\n",
    "\n",
    "    # Load the best weights after training\n",
    "    rnn_seq_model.load_weights(model_path)\n",
    "\n",
    "    # Evaluate the model on a test sample\n",
    "    print(\"\\nEvaluating the model...\")\n",
    "    _, accuracy = rnn_seq_model.evaluate([test_data[0], test_data[1]], test_labels)\n",
    "    print(f\"Test accuracy: {round(accuracy * 100, 2)}%\")\n",
    "    \n",
    "    if HYPERPARAMETER_TUNING_ENABLED:\n",
    "        wandb.log({\"test_accuracy\": accuracy})\n",
    "        \n",
    "    # Log metrics to W&B if W&B is active\n",
    "    if config is not None:\n",
    "        wandb.log({\n",
    "            \"train_loss\": history.history[\"loss\"],\n",
    "            \"val_loss\": history.history[\"val_loss\"],\n",
    "            \"train_accuracy\": history.history[\"accuracy\"],\n",
    "            \"val_accuracy\": history.history[\"val_accuracy\"],\n",
    "        })\n",
    "        \n",
    "        # Finish W&B run\n",
    "        wandb.finish()\n",
    "\n",
    "    return rnn_seq_model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_and_evaluate_model(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def start_training():\n",
    "#     if HYPERPARAMETER_TUNING_ENABLED:\n",
    "#         return wandb.agent(sweep_id, train_and_evaluate_model, count=5)\n",
    "#     else:\n",
    "#         return train_and_evaluate_model(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# sequence_model, history = train_and_evaluate_model(rnn_seq_model)\n",
    "model, history = start_training()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.utils.plot_model(\n",
    "    model,\n",
    "    to_file=f'{model_path}/model.png', #TODO: Fix this\n",
    "    show_shapes=True,\n",
    "    show_layer_names=True,\n",
    "    show_layer_activations=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(model_path):\n",
    "    print(f\"Model saved at: {model_path}\")\n",
    "    model_size = os.path.getsize(model_path) / (1024 * 1024)\n",
    "    print(f\"Model size: {model_size:.2f} MB\")\n",
    "else:\n",
    "    print(\"Model file not found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualise the training and validation loss and accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_training_metrics(history, val_metrics_path):\n",
    "    train_loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "    train_accuracy = history.history['accuracy']\n",
    "    val_accuracy = history.history['val_accuracy']\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    # Loss plot\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(train_loss, label='Training Loss')\n",
    "    plt.plot(val_loss, label='Validation Loss')\n",
    "    plt.title(\"Training and Validation Loss\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()\n",
    "\n",
    "    # Accuracy plot\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(train_accuracy, label='Training Accuracy')\n",
    "    plt.plot(val_accuracy, label='Validation Accuracy')\n",
    "    plt.title(\"Training and Validation Accuracy\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.savefig(val_metrics_path)\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_image_path(metric_type):\n",
    "    image_name = None\n",
    "    \n",
    "    if metric_type == \"Loss\":\n",
    "        image_name = f\"loss.png\"\n",
    "    elif metric_type == \"Confusion Matrix\":\n",
    "        image_name = f\"confusion_matrix.png\"\n",
    "    \n",
    "    image_path = os.path.join(model_base_path, image_name)\n",
    "    print(f'{metric_type} plot saved at: {image_path}')\n",
    "    \n",
    "    return image_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = get_model_image_path(\"Loss\")\n",
    "visualize_training_metrics(history, image_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_features, test_masks = test_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate on the entire dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, accuracy = sequence_model.evaluate([test_features, test_masks], test_labels, batch_size=32)\n",
    "\n",
    "print(f\"Test Loss: {loss :.4f}\")\n",
    "print(f\"Test Accuracy: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation with single sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to prepare the test video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_single_video(frames):\n",
    "    frames = frames[None, ...]\n",
    "    frame_mask = np.zeros(shape=(1, MAX_SEQ_LENGTH, ),dtype=\"bool\" )\n",
    "    frame_features = np.zeros(shape=(1, MAX_SEQ_LENGTH, NUM_FEATURES), dtype=\"float32\")\n",
    "\n",
    "    for i, frames_batch in enumerate(frames):\n",
    "        video_length = frames_batch.shape[0]\n",
    "        length = min(MAX_SEQ_LENGTH, video_length)\n",
    "        for j in range(length):\n",
    "            frame_features[i, j, :] = feature_extractor.predict(frames_batch[None, j, :])\n",
    "        frame_mask[i, :length] = 1  # 1 = not masked, 0 = masked\n",
    "\n",
    "    return frame_features, frame_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make predictions on a single video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sequence_prediction(seq_model, frames, true_label):\n",
    "    frame_features, frame_mask = create_video_mask_and_features(frames)\n",
    "    probabilities = seq_model.predict([frame_features, frame_mask])[0]\n",
    "        \n",
    "    print(\"\\nTop-5 predicted actions:\")\n",
    "    for i in np.argsort(probabilities)[::-1][:5]:\n",
    "        print(f\"  - {class_vocab[i]}: {probabilities[i] * 100:5.2f}%\")\n",
    "    \n",
    "    # Get the top-1 predicted label (highest probability)\n",
    "    predicted_index = np.argmax(probabilities)\n",
    "    predicted_label = class_vocab[predicted_index]\n",
    "\n",
    "    # Convert true_label to index to align with predicted index format\n",
    "    true_label_index = class_vocab.index(true_label)\n",
    "\n",
    "    # Prepare y_true and y_pred as binary arrays (1 for correct label, 0 for others)\n",
    "    y_true = np.zeros(len(class_vocab))\n",
    "    y_pred = np.zeros(len(class_vocab))\n",
    "    y_true[true_label_index] = 1\n",
    "    y_pred[predicted_index] = 1\n",
    " \n",
    "    return y_true, y_pred, predicted_label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display predicted image as GIF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    }
   },
   "outputs": [],
   "source": [
    "def display_as_gif(images):\n",
    "    gif_path = \"../data/animation.gif\"\n",
    "    converted_images = images.astype(np.uint8)\n",
    "    imageio.mimsave(gif_path, converted_images, duration=100)\n",
    "    return Image(gif_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choosing a random video to use for prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_index = np.random.randint(len(test_df))\n",
    "\n",
    "# Get the test video path\n",
    "test_video = test_df[\"video_path\"].values[random_index]\n",
    "\n",
    "# Get the true label of the test video\n",
    "true_label_index = test_labels.tolist()[random_index][0]\n",
    "true_label = class_vocab[true_label_index]\n",
    "\n",
    "test_video_frames = load_and_preprocess_video(test_video)\n",
    "\n",
    "print(f\"Test video path: {test_video}\")\n",
    "print(f\"Label: {true_label}\")\n",
    "\n",
    "print(f\"\"\"\n",
    "Test video frames shape:\n",
    "  - {test_video_frames.shape[0]} frames\n",
    "  - {test_video_frames.shape[1]} pixels (height) x {test_video_frames.shape[2]} pixels (width)\n",
    "  - {test_video_frames.shape[3]} color channels\n",
    "\"\"\")\n",
    "\n",
    "display_as_gif(test_video_frames[:MAX_SEQ_LENGTH])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the prediction on the test video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true, y_pred, predicted_label = sequence_prediction(sequence_model, test_video_frames, true_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\nTrue label: {true_label}\")\n",
    "print(f\"Predicted label: {predicted_label}\")\n",
    "\n",
    "print(f\"y_true: {y_true}\")\n",
    "print(f\"y_pred: {y_pred}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation on the entire test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utility to display evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_evaluation_metrics(y_true, y_pred, predictions, target_classes):\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    print(f\"Top-1 Accuracy: {accuracy * 100:.2f}%\")\n",
    "\n",
    "    k = 2\n",
    "    top_k_acc = top_k_accuracy_score(y_true, predictions, k=k)\n",
    "    print(f\"Top-{k} Accuracy: {top_k_acc * 100:.2f}%\")\n",
    "\n",
    "    report = classification_report(y_true, y_pred, target_names=target_classes, zero_division=0)\n",
    "    print(\"\\nClassification Report:\\n\", report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = sequence_model.predict([test_features, test_masks], batch_size=32)\n",
    "predicted_classes = np.argmax(predictions, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_evaluation_metrics(test_labels, predicted_classes, predictions, class_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_confusion_matrix(y_true, y_pred, target_classes, plot_path, show_plot=False):\n",
    "    conf_matrix = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(\n",
    "        conf_matrix,\n",
    "        fmt=\"d\",\n",
    "        annot=True,\n",
    "        cmap=\"Blues\",\n",
    "        cbar=True,\n",
    "        xticklabels=target_classes,\n",
    "        yticklabels=target_classes,\n",
    "    )\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"True\")\n",
    "    plt.title(\"Confusion Matrix\")\n",
    "\n",
    "    plt.savefig(plot_path)\n",
    "\n",
    "    if show_plot:\n",
    "        plt.show()    \n",
    "    else:\n",
    "        print(conf_matrix)\n",
    "\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_labels = np.array(test_labels).flatten()\n",
    "cm_plot_path = get_model_image_path(\"Confusion Matrix\")\n",
    "display_confusion_matrix(true_labels, predicted_classes, class_vocab, cm_plot_path, show_plot=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> K. Soomro, A. R. Zamir, and M. Shah, \"UCF101: A Dataset of 101 Human Actions Classes From Videos in The Wild,\" arXiv, 2012. [Online]. Available: https://arxiv.org/abs/1212.0402\n",
    "\n",
    "> A. Arnab, M. Dehghani, G. Heigold, C. Sun, M. Lučić, and C. Schmid, “ViVIT: a video vision transformer,” arXiv.org, Mar. 29, 2021. https://arxiv.org/abs/2103.15691\n",
    "\n",
    "> X. Liu, Y. Shen, J. Liu, J. Yang, P. Xiong, and F. Lin, “Parallel Spatial–Temporal Self-Attention CNN-Based Motor Imagery Classification for BCI,” Frontiers in Neuroscience, vol. 14, Dec. 2020, doi: 10.3389/fnins.2020.587520.\n",
    "\n",
    "> C. Szegedy, V. Vanhoucke, S. Ioffe, J. Shlens, and Z. Wojna, \"Rethinking the Inception Architecture for Computer Vision,\" arXiv preprint arXiv:1512.00567, 2015.\n",
    "\n",
    "> Singh, S., Dewangan, S., Krishna, G., Tyagi, V., & Reddy, S. (2022). Video vision transformers for violence detection. arXiv. https://doi.org/10.48550/arXiv.2209.03561"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 4915922,
     "datasetId": 2807884,
     "sourceId": 4849320,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30787,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
