{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sports Action Recognition Using an I3D(`Inflated 3D ConvNet`) Architecture on the UCF101 10 Sports actions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-11-06T02:10:51.359159Z",
     "iopub.status.busy": "2024-11-06T02:10:51.358754Z",
     "iopub.status.idle": "2024-11-06T02:11:09.254053Z",
     "shell.execute_reply": "2024-11-06T02:11:09.253134Z",
     "shell.execute_reply.started": "2024-11-06T02:10:51.359087Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    top_k_accuracy_score,\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    ")\n",
    "\n",
    "import imageio\n",
    "import cv2\n",
    "from IPython.display import Image\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from tensorflow.keras.metrics import top_k_categorical_accuracy\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import (\n",
    "    Conv3D,\n",
    "    MaxPool3D,\n",
    "    BatchNormalization,\n",
    "    Input,\n",
    "    Flatten,\n",
    "    Dense,\n",
    "    Dropout,\n",
    "    GRU,\n",
    "    Bidirectional,\n",
    "    GlobalAveragePooling3D\n",
    ")\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "\n",
    "import kagglehub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download latest version of the ucf101-action-recognition dataset from Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = kagglehub.dataset_download(\"matthewjansen/ucf101-action-recognition\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-06T02:12:09.427435Z",
     "iopub.status.busy": "2024-11-06T02:12:09.427055Z",
     "iopub.status.idle": "2024-11-06T02:12:09.431825Z",
     "shell.execute_reply": "2024-11-06T02:12:09.430893Z",
     "shell.execute_reply.started": "2024-11-06T02:12:09.427384Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(\"Path to dataset files: \\n\", path)\n",
    "print(\"\\nFiles in dataset directory:\\n\", os.listdir(path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Class Definition**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sports_actions = [\n",
    "    \"SkyDiving\",\n",
    "    \"Biking\",\n",
    "    \"HorseRace\",\n",
    "    \"Surfing\",\n",
    "    \"TennisSwing\",\n",
    "    \"Punch\",\n",
    "    \"Basketball\",\n",
    "    \"JumpRope\",\n",
    "    \"Archery\",\n",
    "    \"Skiing\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utility to transform video paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(dataset_type):\n",
    "    dataset_path = os.path.join(path, f\"{dataset_type}.csv\")\n",
    "    dataset = pd.read_csv(dataset_path)[0:100]\n",
    "\n",
    "    # Filter dataset to only include the specified sports actions\n",
    "    filtered_dataset = dataset[dataset[\"label\"].isin(sports_actions)]\n",
    "\n",
    "    return pd.DataFrame(\n",
    "        {\n",
    "            \"label\": filtered_dataset[\"label\"],\n",
    "            \"video_path\": filtered_dataset[\"clip_path\"].apply(lambda x: f\"{path}{x}\"),\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the train and test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = load_dataset(\"train\")\n",
    "val_df = load_dataset(\"val\")\n",
    "test_df = load_dataset(\"test\")\n",
    "\n",
    "print(f\"Total videos for training: {len(train_df)}\")\n",
    "print(f\"Total videos for validation: {len(val_df)}\")\n",
    "print(f\"Total videos for testing: {len(test_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of unique classes in training set: \", len(train_df[\"label\"].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configs & Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-06T02:12:09.480471Z",
     "iopub.status.busy": "2024-11-06T02:12:09.480187Z",
     "iopub.status.idle": "2024-11-06T02:12:09.484927Z",
     "shell.execute_reply": "2024-11-06T02:12:09.484040Z",
     "shell.execute_reply.started": "2024-11-06T02:12:09.480440Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "MODEL_NAME = \"3d_ucf101_10c_v2_i3d_tl\"\n",
    "MODEL_BASE_PATH = f\"../../models/{MODEL_NAME}\"\n",
    "\n",
    "FRAME_SIZE = 224\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 60\n",
    "LEARNING_RATE = 1e-4\n",
    "\n",
    "MAX_SEQ_LENGTH = 20\n",
    "NUM_FEATURES = 512"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Label Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encode the class labels as integers using the Keras StringLookup layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(train_df[\"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_processor = tf.keras.layers.StringLookup(num_oov_indices=0, vocabulary=sports_actions)\n",
    "\n",
    "class_vocab = label_processor.get_vocabulary()\n",
    "\n",
    "print(f\"Vocabulary: {class_vocab}\")\n",
    "print(f\"Number of classes: {len(class_vocab)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utility to convert string labels to one-hot encoded format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_labels(labels: np.ndarray) -> np.ndarray:\n",
    "    integer_labels = tf.keras.ops.convert_to_numpy(label_processor(labels[..., None]))\n",
    "    return tf.keras.utils.to_categorical(integer_labels, num_classes=len(class_vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Video preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Function to resize the video frames to a square shape without distorting their content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-06T02:12:09.486337Z",
     "iopub.status.busy": "2024-11-06T02:12:09.486016Z",
     "iopub.status.idle": "2024-11-06T02:12:09.496479Z",
     "shell.execute_reply": "2024-11-06T02:12:09.495613Z",
     "shell.execute_reply.started": "2024-11-06T02:12:09.486305Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def crop_center_square(frame):\n",
    "    y, x = frame.shape[0:2]  # Get the height (y) and width (x) of the image\n",
    "    min_dim = min(y, x)       # Find the smallest dimension (either height or width)\n",
    "    start_x = (x // 2) - (min_dim // 2)  # Calculate the horizontal starting point for the crop\n",
    "    start_y = (y // 2) - (min_dim // 2)  # Calculate the vertical starting point for the crop\n",
    "\n",
    "    return frame[start_y : start_y + min_dim, start_x : start_x + min_dim]  # Return the cropped square"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Crop,resize, and reorder color channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_preprocess_video(video_path):    \n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    frames = []\n",
    "    try:\n",
    "        while True:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            frame = crop_center_square(frame) # Crop center square\n",
    "            frame = cv2.resize(frame, (FRAME_SIZE,FRAME_SIZE)) # Resize the image (In this case to 224x224)\n",
    "            frame = frame[:, :, [2, 1, 0]] # Reorder the color channels from OpenCV BGR to RGB\n",
    "            frame = frame.astype('float32') / 255.0 # Normalize the pixel values\n",
    "            \n",
    "            # For Pre-trained I3D normalization \n",
    "            # mean = [0.485, 0.456, 0.406]  # For RGB channels\n",
    "            # std = [0.229, 0.224, 0.225]   # For RGB channels\n",
    "            # frame = (frame - mean) / std\n",
    "\n",
    "            frames.append(frame)\n",
    "\n",
    "            if len(frames) == MAX_SEQ_LENGTH:\n",
    "                break\n",
    "    finally:\n",
    "        cap.release()\n",
    "   \n",
    "    return np.array(frames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the video frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_video_frames(video_paths):\n",
    "    frames_list = []\n",
    "\n",
    "    for video_path in video_paths:\n",
    "        frames = load_and_preprocess_video(video_path)\n",
    "\n",
    "        # If the video has more frames than max_video_length, sample frames\n",
    "        if frames.shape[0] > MAX_SEQ_LENGTH:\n",
    "            frame_indices = np.linspace(0, frames.shape[0] - 1, MAX_SEQ_LENGTH, dtype=int)\n",
    "            frames = frames[frame_indices]\n",
    "\n",
    "        # If the video is too short, replicate frames\n",
    "        elif frames.shape[0] < MAX_SEQ_LENGTH:\n",
    "            repeat_factor = MAX_SEQ_LENGTH // frames.shape[0] + 1\n",
    "            frames = np.tile(frames, (repeat_factor, 1, 1, 1))[:MAX_SEQ_LENGTH]\n",
    "\n",
    "        frames_list.append(frames)\n",
    "\n",
    "    return np.array(frames_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A generator that yields batches of video frames and labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_generator(df, batch_size=32):\n",
    "    video_paths = df[\"video_path\"].values\n",
    "    labels = tf.keras.ops.convert_to_numpy(label_processor(train_df[\"label\"].values[..., None]))\n",
    "    \n",
    "    num_samples = len(video_paths)\n",
    "\n",
    "    while True:\n",
    "        for i in range(0, num_samples, batch_size):\n",
    "            batch_video_paths = video_paths[i : i + batch_size]\n",
    "            batch_labels = labels[i : i + batch_size]\n",
    "\n",
    "            batch_frames = load_video_frames(batch_video_paths)\n",
    "            \n",
    "            yield batch_frames, batch_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inflated 3D Model Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inflated 3D Convnet model trained for action recognition on Kinetics data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = kagglehub.model_download(\"deepmind/i3d-kinetics/tensorFlow1/400\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path to model files: /Users/mzitoh/.cache/kagglehub/models/deepmind/i3d-kinetics/tensorFlow1/400/1\n",
      "Path to model files: /Users/mzitoh/.cache/kagglehub/models/deepmind/i3d-kinetics/tensorFlow1/400/1/saved_model.pb\n"
     ]
    }
   ],
   "source": [
    "print(\"Path to model files:\", model_path)\n",
    "\n",
    "ff_path = os.path.join(model_path, \"saved_model.pb\")\n",
    "\n",
    "print(\"Path to model files:\", ff_path)\n",
    "\n",
    "\n",
    "# /Users/mzitoh/.cache/kagglehub/models/deepmind/i3d-kinetics/tensorFlow1/400/1/saved_model.pb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_i3d_model():\n",
    "    # Load the pre-trained I3D model from Kaggle\n",
    "    i3d_model = tf.keras.models.load_model(f\"{model_path}/saved_model.pb\")\n",
    "\n",
    "    # Freeze all layers in the pre-trained I3D model initially\n",
    "    i3d_model.trainable = False\n",
    "\n",
    "    # Create the input layer for the video clips\n",
    "    video_input = Input(shape=(MAX_SEQ_LENGTH, FRAME_SIZE, FRAME_SIZE, 3))\n",
    "\n",
    "    # Pass the video input through the pre-trained I3D model\n",
    "    x = i3d_model(video_input)\n",
    "\n",
    "    # Global average pooling to reduce the 3D features to a 2D representation\n",
    "    x = GlobalAveragePooling3D()(x)\n",
    "\n",
    "    # Add some dense layers for further classification\n",
    "    x = Dense(128, activation=\"relu\")(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = Dense(64, activation=\"relu\")(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "\n",
    "    # Output layer with softmax activation for multi-class classification\n",
    "    output = Dense(len(class_vocab), activation=\"softmax\")(x)\n",
    "\n",
    "    # Create the model\n",
    "    model = Model(inputs=video_input, outputs=output, name=\"i3d_model\")\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),\n",
    "        loss=\"sparse_categorical_crossentropy\",\n",
    "        metrics=[\"accuracy\"],\n",
    "    )\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Fine-tune the X3D model\n",
    "# def fine_tune_x3d_model(model, train_generator, val_generator, epochs=10):\n",
    "#     # Unfreeze the top layers for fine-tuning\n",
    "#     for layer in model.layers[:60]:  # Unfreeze the first 60 layers (adjust as needed)\n",
    "#         layer.trainable = False\n",
    "    \n",
    "#     for layer in model.layers[60:]:  # Unfreeze the remaining layers\n",
    "#         layer.trainable = True\n",
    "\n",
    "#     # Compile the model again with a lower learning rate\n",
    "#     model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001), \n",
    "#                   loss='sparse_categorical_crossentropy', \n",
    "#                   metrics=['accuracy'])\n",
    "\n",
    "#     # Fine-tune the model on the training data\n",
    "#     model.fit(train_generator, epochs=epochs, validation_data=val_generator)\n",
    "    \n",
    "#     return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_x3d_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_new_model_version():\n",
    "    model_version = 1\n",
    "    while os.path.exists(f\"{MODEL_BASE_PATH}/v_{model_version}\"):\n",
    "        model_version += 1\n",
    "    return model_version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_version_path():\n",
    "    model_version = get_new_model_version()\n",
    "    model_version_path = f\"{MODEL_BASE_PATH}/v_{model_version}\"\n",
    "\n",
    "    return model_version_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_path(model_version_path):\n",
    "    model_path = os.path.join(model_version_path, f\"{MODEL_NAME}.keras\")\n",
    "    \n",
    "    return model_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utility to run the experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, model_path):\n",
    "    batch_size = 32\n",
    "\n",
    "    callbacks = [\n",
    "        ModelCheckpoint(\n",
    "            model_path,\n",
    "            save_weights_only=False,\n",
    "            save_best_only=True,\n",
    "            monitor=\"val_accuracy\",\n",
    "            verbose=1,\n",
    "        ),\n",
    "        EarlyStopping(monitor=\"val_loss\", patience=10, restore_best_weights=True),\n",
    "    ]\n",
    "\n",
    "    train_gen = data_generator(train_df, batch_size=batch_size)\n",
    "    val_gen = data_generator(val_df, batch_size=batch_size)\n",
    "\n",
    "    history = model.fit(\n",
    "        train_gen,\n",
    "        validation_data=val_gen,\n",
    "        steps_per_epoch=len(train_df) // batch_size,\n",
    "        validation_steps=len(val_df) // batch_size,\n",
    "        epochs=EPOCHS,\n",
    "        # batch_size=BATCH_SIZE,\n",
    "        callbacks=callbacks,\n",
    "        verbose=1,\n",
    "    )\n",
    "\n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_version_path = get_model_version_path()\n",
    "model_path = get_model_path(model_version_path)\n",
    "\n",
    "x3d_model, history = train_model(model, model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(model_path):\n",
    "    print(f\"Model saved at: {model_path}\")\n",
    "    model_size = os.path.getsize(model_path) / (1024 * 1024)\n",
    "    print(f\"Model size: {model_size:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualise the training and validation loss and accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_training_metrics(history, metrics_path):\n",
    "    train_loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "    train_accuracy = history.history['accuracy']\n",
    "    val_accuracy = history.history['val_accuracy']\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    # Loss plot\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(train_loss, label='Training Loss')\n",
    "    plt.plot(val_loss, label='Validation Loss')\n",
    "    plt.title(\"Training and Validation Loss\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()\n",
    "\n",
    "    # Accuracy plot\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(train_accuracy, label='Training Accuracy')\n",
    "    plt.plot(val_accuracy, label='Validation Accuracy')\n",
    "    plt.title(\"Training and Validation Accuracy\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.savefig(metrics_path)\n",
    "    print(f\"Training metrics plot saved at: {metrics_path}\")\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_image_path(metric_type, model_version_path):\n",
    "    image_path = f\"{model_version_path}/{metric_type}.png\"\n",
    "    return image_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = get_model_image_path(\"loss\", model_version_path)\n",
    "visualize_training_metrics(history, image_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation and Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate on the entire test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_gen = data_generator(test_df, batch_size=32)\n",
    "\n",
    "x3d_model.load_weights(model_path) # Load the best weights\n",
    "test_loss, test_accuracy, test_top_k = x3d_model.evaluate(test_gen, steps=len(test_df) // 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Test Loss: {test_loss :.4f}\")\n",
    "print(f\"Test Accuracy: {test_accuracy * 100:.2f}%\")\n",
    "print(f\"Test Top-5 Accuracy: {test_top_k * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation with single sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make predictions on a single video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, video_frames, true_label):\n",
    "    # Expand the dimensions to match model input shape\n",
    "    video_frames = np.expand_dims(\n",
    "        video_frames, axis=0\n",
    "    )  # Shape (1, num_frames, height, width, channels)\n",
    "\n",
    "    # Get model predictions\n",
    "    y_pred = model.predict(video_frames)\n",
    "\n",
    "    # Get the predicted label (index of the highest probability)\n",
    "    predicted_label_index = np.argmax(y_pred, axis=1)[0]\n",
    "    predicted_label = class_vocab[predicted_label_index]  # Map index to label\n",
    "\n",
    "    print(f\"Prediction probabilities: {y_pred}\")\n",
    "    print(f\"Predicted label index: {predicted_label_index}\")\n",
    "\n",
    "    return true_label, y_pred, predicted_label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display predicted image as GIF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_as_gif(frames, model_version_path, save=False):\n",
    "    gif_path = f\"{model_version_path}/test_animation.gif\"\n",
    "    converted_images = frames.astype(np.uint8)\n",
    "\n",
    "    if save:\n",
    "        imageio.mimsave(gif_path, converted_images, duration=100)\n",
    "        print(f\"GIF saved at {gif_path}\")\n",
    "\n",
    "    return Image.open(gif_path)  # Display the gif"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choosing a random video to use for prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_test_video(test_df, test_labels, save_gif=False):\n",
    "    # Select a random test video\n",
    "    random_index = np.random.randint(len(test_df))\n",
    "    \n",
    "    # Get the test video path\n",
    "    test_video = test_df[\"video_path\"].values[random_index]\n",
    "\n",
    "    # Get the true label of the test video\n",
    "    true_label_index = test_labels.tolist()[random_index][0]\n",
    "    true_label = class_vocab[true_label_index]\n",
    "\n",
    "    # Load video frames\n",
    "    test_video_frames = load_video_frames(test_video)\n",
    "\n",
    "    print(f\"Test video path: {test_video}\")\n",
    "    print(f\"Label: {true_label}\")\n",
    "    \n",
    "    # Display the shape of the test video frames\n",
    "    print(f\"\"\"\n",
    "    Test video frames shape:\n",
    "      - {test_video_frames.shape[0]} frames\n",
    "      - {test_video_frames.shape[1]} pixels (height) x {test_video_frames.shape[2]} pixels (width)\n",
    "      - {test_video_frames.shape[3]} color channels\n",
    "    \"\"\")\n",
    "\n",
    "    display_as_gif(test_video_frames[:MAX_SEQ_LENGTH], model_version_path, save_gif)\n",
    "\n",
    "    return test_video_frames, true_label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the prediction on the test video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_labels = encode_labels(test_df[\"label\"].values)\n",
    "\n",
    "test_video_frames, true_label = get_test_video(test_df, test_labels, save_gif=True)\n",
    "y_true, y_pred, predicted_label = predict(x3d_model, test_video_frames, true_label)\n",
    "\n",
    "print(f\"\\nTrue label: {true_label}\")\n",
    "print(f\"Predicted label: {predicted_label}\")\n",
    "\n",
    "print(f\"y_true: {y_true}\")\n",
    "print(f\"y_pred: {y_pred}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_test_videos(model, test_gen, batch_size=32):\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "\n",
    "    # Calculate the number of steps per epoch\n",
    "    steps = len(test_gen) // batch_size\n",
    "\n",
    "    for i in range(steps):\n",
    "        # Get a batch of frames and labels\n",
    "        batch_frames, batch_labels = next(test_gen)\n",
    "\n",
    "        # Predict using the model\n",
    "        predictions = model.predict(batch_frames, batch_size=batch_size)\n",
    "\n",
    "        # Get the predicted class labels\n",
    "        batch_pred_labels = np.argmax(predictions, axis=1)\n",
    "\n",
    "        # Append the true labels and predicted labels\n",
    "        # Convert one-hot labels to integers\n",
    "        y_true.extend(np.argmax(batch_labels, axis=1))\n",
    "        y_pred.extend(batch_pred_labels)\n",
    "\n",
    "        # Print progress every 100 batches\n",
    "        if i % 50 == 0:\n",
    "            print(f\"Processed {i+1}/{steps} batches\")\n",
    "\n",
    "    return np.array(y_true), np.array(y_pred), predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utility to save classification metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_classification_report(report, save_path):\n",
    "    report_data = []\n",
    "    lines = report.split(\"\\n\")\n",
    "\n",
    "    for line in filter(None, lines[2:-3]):  # Remove empty lines and headers/footers\n",
    "        row = line.split()\n",
    "        name = row[0]\n",
    "        stats = row[1:]\n",
    "\n",
    "        # Convert stats to float, handling support as int\n",
    "        stats = [float(val) for val in stats[:-1]] + [int(stats[-1])]\n",
    "        report_data.append([name] + stats)\n",
    "\n",
    "    report_df = pd.DataFrame(\n",
    "        report_data, columns=[\"Class\", \"Precision\", \"Recall\", \"F1-score\", \"Support\"]\n",
    "    )\n",
    "\n",
    "    report_df.to_csv(save_path, index=False)\n",
    "\n",
    "    print(f\"Classification report saved at: {save_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utility to display evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_evaluation_metrics(y_true, y_pred, predictions, target_classes):\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    print(f\"Top-1 Accuracy: {accuracy * 100:.2f}%\")\n",
    "\n",
    "    k = 2\n",
    "    top_k_acc = top_k_accuracy_score(y_true, predictions, k=k)\n",
    "    print(f\"Top-{k} Accuracy: {top_k_acc * 100:.2f}%\")\n",
    "\n",
    "    report = classification_report(y_true, y_pred, target_names=target_classes)\n",
    "    print(\"\\nClassification Report:\\n\", report)\n",
    "\n",
    "    save_classification_report(report, f'{model_version_path}/classification_metrics.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true, y_pred, predictions = predict_test_videos(x3d_model, test_gen)\n",
    "\n",
    "# y_pred = np.argmax(predictions, axis=1)\n",
    "\n",
    "# Print the results\n",
    "print(f\"True Labels: {y_true}\")\n",
    "print(f\"Predicted Labels: {y_pred}\")\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = np.sum(y_true == y_pred) / len(y_true)\n",
    "print(f\"Test Accuracy: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_evaluation_metrics(y_true, y_pred, predictions, class_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_confusion_matrix(y_true, y_pred, target_classes, plot_path, show_plot=False):\n",
    "    conf_matrix = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(\n",
    "        conf_matrix,\n",
    "        fmt=\"d\",\n",
    "        annot=True,\n",
    "        cmap=\"Blues\",\n",
    "        cbar=True,\n",
    "        xticklabels=target_classes,\n",
    "        yticklabels=target_classes,\n",
    "    )\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"True\")\n",
    "    plt.title(\"Confusion Matrix\")\n",
    "\n",
    "    if show_plot:\n",
    "        plt.show()    \n",
    "    else:\n",
    "        print(conf_matrix)\n",
    "\n",
    "    plt.savefig(plot_path)\n",
    "    print(f\"\\nConfusion matrix saved at: {plot_path}\")\n",
    "\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_labels = np.array(test_labels).flatten()\n",
    "\n",
    "cm_plot_path = get_model_image_path(\"confusion_matrix\", model_version_path)\n",
    "display_confusion_matrix(true_labels, y_pred, class_vocab, cm_plot_path, show_plot=False)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 4915922,
     "datasetId": 2807884,
     "sourceId": 4849320,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30787,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
