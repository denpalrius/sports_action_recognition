{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sports Action Recognition Using an I3D(`Inflated 3D ConvNet`) Architecture on the UCF101 10 Sports actions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-11-06T02:10:51.359159Z",
     "iopub.status.busy": "2024-11-06T02:10:51.358754Z",
     "iopub.status.idle": "2024-11-06T02:11:09.254053Z",
     "shell.execute_reply": "2024-11-06T02:11:09.253134Z",
     "shell.execute_reply.started": "2024-11-06T02:10:51.359087Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    top_k_accuracy_score,\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    ")\n",
    "\n",
    "import imageio\n",
    "import cv2\n",
    "from IPython.display import Image\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from tensorflow.keras.metrics import top_k_categorical_accuracy\n",
    "from tensorflow.keras.layers import Dense, Input, Flatten\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import (\n",
    "    Conv3D,\n",
    "    MaxPool3D,\n",
    "    BatchNormalization,\n",
    "    Input,\n",
    "    Flatten,\n",
    "    Dense,\n",
    "    Dropout,\n",
    "    GRU,\n",
    "    Bidirectional,\n",
    ")\n",
    "\n",
    "import kagglehub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download latest version of the ucf101-action-recognition dataset from Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = kagglehub.dataset_download(\"matthewjansen/ucf101-action-recognition\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-06T02:12:09.427435Z",
     "iopub.status.busy": "2024-11-06T02:12:09.427055Z",
     "iopub.status.idle": "2024-11-06T02:12:09.431825Z",
     "shell.execute_reply": "2024-11-06T02:12:09.430893Z",
     "shell.execute_reply.started": "2024-11-06T02:12:09.427384Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path to dataset files: \n",
      " /Users/mzitoh/.cache/kagglehub/datasets/matthewjansen/ucf101-action-recognition/versions/4\n",
      "\n",
      "Files in dataset directory:\n",
      " ['test', 'val.csv', 'test.csv', 'train', 'train.csv', 'val']\n"
     ]
    }
   ],
   "source": [
    "print(\"Path to dataset files: \\n\", path)\n",
    "print(\"\\nFiles in dataset directory:\\n\", os.listdir(path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Class Definition**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sports_actions = [\n",
    "    \"SkyDiving\",\n",
    "    \"Biking\",\n",
    "    \"HorseRace\",\n",
    "    \"Surfing\",\n",
    "    \"TennisSwing\",\n",
    "    \"Punch\",\n",
    "    \"Basketball\",\n",
    "    \"JumpRope\",\n",
    "    \"Archery\",\n",
    "    \"Skiing\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utility to transform video paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(dataset_type):\n",
    "    dataset_path = os.path.join(path, f\"{dataset_type}.csv\")\n",
    "    dataset = pd.read_csv(dataset_path)\n",
    "\n",
    "    # Filter dataset to only include the specified sports actions\n",
    "    filtered_dataset = dataset[dataset[\"label\"].isin(sports_actions)]\n",
    "\n",
    "    return pd.DataFrame(\n",
    "        {\n",
    "            \"label\": filtered_dataset[\"label\"],\n",
    "            \"video_path\": filtered_dataset[\"clip_path\"].apply(lambda x: f\"{path}{x}\"),\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the train and test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total videos for training: 1128\n",
      "Total videos for validation: 189\n",
      "Total videos for testing: 192\n"
     ]
    }
   ],
   "source": [
    "train_df = load_dataset(\"train\")\n",
    "val_df = load_dataset(\"val\")\n",
    "test_df = load_dataset(\"test\")\n",
    "\n",
    "print(f\"Total videos for training: {len(train_df)}\")\n",
    "print(f\"Total videos for validation: {len(val_df)}\")\n",
    "print(f\"Total videos for testing: {len(test_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique classes in training set:  10\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of unique classes in training set: \", len(train_df[\"label\"].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>video_path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>SkyDiving</td>\n",
       "      <td>/Users/mzitoh/.cache/kagglehub/datasets/matthe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>SkyDiving</td>\n",
       "      <td>/Users/mzitoh/.cache/kagglehub/datasets/matthe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3011</th>\n",
       "      <td>Archery</td>\n",
       "      <td>/Users/mzitoh/.cache/kagglehub/datasets/matthe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4464</th>\n",
       "      <td>JumpRope</td>\n",
       "      <td>/Users/mzitoh/.cache/kagglehub/datasets/matthe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2846</th>\n",
       "      <td>Punch</td>\n",
       "      <td>/Users/mzitoh/.cache/kagglehub/datasets/matthe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2962</th>\n",
       "      <td>Archery</td>\n",
       "      <td>/Users/mzitoh/.cache/kagglehub/datasets/matthe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2928</th>\n",
       "      <td>Punch</td>\n",
       "      <td>/Users/mzitoh/.cache/kagglehub/datasets/matthe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>787</th>\n",
       "      <td>TennisSwing</td>\n",
       "      <td>/Users/mzitoh/.cache/kagglehub/datasets/matthe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9801</th>\n",
       "      <td>Surfing</td>\n",
       "      <td>/Users/mzitoh/.cache/kagglehub/datasets/matthe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4512</th>\n",
       "      <td>JumpRope</td>\n",
       "      <td>/Users/mzitoh/.cache/kagglehub/datasets/matthe...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            label                                         video_path\n",
       "103     SkyDiving  /Users/mzitoh/.cache/kagglehub/datasets/matthe...\n",
       "152     SkyDiving  /Users/mzitoh/.cache/kagglehub/datasets/matthe...\n",
       "3011      Archery  /Users/mzitoh/.cache/kagglehub/datasets/matthe...\n",
       "4464     JumpRope  /Users/mzitoh/.cache/kagglehub/datasets/matthe...\n",
       "2846        Punch  /Users/mzitoh/.cache/kagglehub/datasets/matthe...\n",
       "2962      Archery  /Users/mzitoh/.cache/kagglehub/datasets/matthe...\n",
       "2928        Punch  /Users/mzitoh/.cache/kagglehub/datasets/matthe...\n",
       "787   TennisSwing  /Users/mzitoh/.cache/kagglehub/datasets/matthe...\n",
       "9801      Surfing  /Users/mzitoh/.cache/kagglehub/datasets/matthe...\n",
       "4512     JumpRope  /Users/mzitoh/.cache/kagglehub/datasets/matthe..."
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configs & Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-06T02:12:09.480471Z",
     "iopub.status.busy": "2024-11-06T02:12:09.480187Z",
     "iopub.status.idle": "2024-11-06T02:12:09.484927Z",
     "shell.execute_reply": "2024-11-06T02:12:09.484040Z",
     "shell.execute_reply.started": "2024-11-06T02:12:09.480440Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "MODEL_NAME = \"i3d_v1_100f\"\n",
    "MODEL_BASE_PATH = f\"../../models/{MODEL_NAME}\"\n",
    "\n",
    "FRAME_SIZE = 224\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 60\n",
    "LEARNING_RATE = 1e-4\n",
    "\n",
    "MAX_SEQ_LENGTH = 20\n",
    "NUM_FEATURES = 512"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Label Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encode the class labels as integers using the Keras StringLookup layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Archery', 'Basketball', 'Biking', 'HorseRace', 'JumpRope',\n",
       "       'Punch', 'Skiing', 'SkyDiving', 'Surfing', 'TennisSwing'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(train_df[\"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary: ['SkyDiving', 'Biking', 'HorseRace', 'Surfing', 'TennisSwing', 'Punch', 'Basketball', 'JumpRope', 'Archery', 'Skiing']\n",
      "Number of classes: 10\n"
     ]
    }
   ],
   "source": [
    "label_processor = tf.keras.layers.StringLookup(num_oov_indices=0, vocabulary=sports_actions)\n",
    "\n",
    "class_vocab = label_processor.get_vocabulary()\n",
    "\n",
    "print(f\"Vocabulary: {class_vocab}\")\n",
    "print(f\"Number of classes: {len(class_vocab)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utility to convert string labels to one-hot encoded format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_labels(labels: np.ndarray) -> np.ndarray:\n",
    "    integer_labels = tf.keras.ops.convert_to_numpy(label_processor(labels[..., None]))\n",
    "    return tf.keras.utils.to_categorical(integer_labels, num_classes=len(class_vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Video preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Function to resize the video frames to a square shape without distorting their content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-06T02:12:09.486337Z",
     "iopub.status.busy": "2024-11-06T02:12:09.486016Z",
     "iopub.status.idle": "2024-11-06T02:12:09.496479Z",
     "shell.execute_reply": "2024-11-06T02:12:09.495613Z",
     "shell.execute_reply.started": "2024-11-06T02:12:09.486305Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def crop_center_square(frame):\n",
    "    y, x = frame.shape[0:2]  # Get the height (y) and width (x) of the image\n",
    "    min_dim = min(y, x)       # Find the smallest dimension (either height or width)\n",
    "    start_x = (x // 2) - (min_dim // 2)  # Calculate the horizontal starting point for the crop\n",
    "    start_y = (y // 2) - (min_dim // 2)  # Calculate the vertical starting point for the crop\n",
    "\n",
    "    return frame[start_y : start_y + min_dim, start_x : start_x + min_dim]  # Return the cropped square"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Crop,resize, and reorder color channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_preprocess_video(video_path):    \n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    frames = []\n",
    "    try:\n",
    "        while True:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            frame = crop_center_square(frame) # Crop center square\n",
    "            frame = cv2.resize(frame, (FRAME_SIZE,FRAME_SIZE)) # Resize the image (In this case to 224x224)\n",
    "            frame = frame[:, :, [2, 1, 0]] # Reorder the color channels from OpenCV BGR to RGB\n",
    "            frame = frame.astype('float32') / 255.0 # Normalize the pixel values\n",
    "            \n",
    "            # For Pre-trained I3D normalization \n",
    "            # mean = [0.485, 0.456, 0.406]  # For RGB channels\n",
    "            # std = [0.229, 0.224, 0.225]   # For RGB channels\n",
    "            # frame = (frame - mean) / std\n",
    "\n",
    "            frames.append(frame)\n",
    "\n",
    "            if len(frames) == MAX_SEQ_LENGTH:\n",
    "                break\n",
    "    finally:\n",
    "        cap.release()\n",
    "   \n",
    "    return np.array(frames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the video frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_single_video(video_path):\n",
    "    print(f\"Loading video from {video_path}\")\n",
    "    if isinstance(video_path, bytes):\n",
    "        video_path = video_path.decode('utf-8')\n",
    "    \n",
    "    # Load and preprocess video frames\n",
    "    frames = load_and_preprocess_video(video_path)\n",
    "    \n",
    "    # Handle sequence length\n",
    "    if frames.shape[0] > MAX_SEQ_LENGTH:\n",
    "        # Sample frames uniformly\n",
    "        frame_indices = np.linspace(0, frames.shape[0] - 1, MAX_SEQ_LENGTH, dtype=int)\n",
    "        frames = frames[frame_indices]\n",
    "    elif frames.shape[0] < MAX_SEQ_LENGTH:\n",
    "        # Repeat frames if video is too short\n",
    "        repeat_factor = MAX_SEQ_LENGTH // frames.shape[0] + 1\n",
    "        frames = np.tile(frames, (repeat_factor, 1, 1, 1))[:MAX_SEQ_LENGTH]\n",
    "    \n",
    "    frames = tf.convert_to_tensor(frames, dtype=tf.float32)\n",
    "    frames.set_shape((MAX_SEQ_LENGTH, FRAME_SIZE, FRAME_SIZE, 3))\n",
    "    \n",
    "    return frames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A generator that yields batches of video frames and labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_video_dataset(df, batch_size=32, shuffle=True, cache=False):\n",
    "    video_paths = df[\"video_path\"].values\n",
    "    labels = tf.keras.ops.convert_to_numpy(\n",
    "        label_processor(df[\"label\"].values[..., None])\n",
    "    )\n",
    "\n",
    "    # Create a dataset of paths and labels\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((video_paths, labels))\n",
    "\n",
    "    if shuffle:\n",
    "        dataset = dataset.shuffle(buffer_size=len(video_paths))\n",
    "\n",
    "    dataset = dataset.map(\n",
    "        lambda x, y: (\n",
    "            tf.py_function(func=load_single_video, inp=[x], Tout=tf.float32),\n",
    "            y,\n",
    "        ),\n",
    "        num_parallel_calls=tf.data.AUTOTUNE,\n",
    "    )\n",
    "\n",
    "    if cache:\n",
    "        dataset = dataset.cache()\n",
    "\n",
    "    # Batch and prefetch\n",
    "    dataset = dataset.batch(batch_size, drop_remainder=True)\n",
    "    dataset = dataset.prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_train_val_datasets(train_df, val_df, batch_size=32):\n",
    "    train_dataset = create_video_dataset(\n",
    "        train_df, batch_size=batch_size, shuffle=True, cache=True\n",
    "    )\n",
    "\n",
    "    val_dataset = create_video_dataset(\n",
    "        val_df, batch_size=batch_size, shuffle=False, cache=True\n",
    "    )\n",
    "\n",
    "    return train_dataset, val_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inflated 3D Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_inflated_3d_model():\n",
    "    # Input layer with explicit batch size if provided\n",
    "    video_input = Input(shape=(MAX_SEQ_LENGTH, FRAME_SIZE, FRAME_SIZE, 3), \n",
    "                       batch_size=BATCH_SIZE,\n",
    "                       name='video_input')\n",
    "    \n",
    "    # Inflated 3D Convolutional layers\n",
    "    x = Conv3D(64, (3, 7, 7), strides=(1, 2, 2), padding=\"same\", activation=\"relu\")(video_input)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Conv3D(128, (3, 3, 3), strides=(1, 2, 2), padding=\"same\", activation=\"relu\")(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Conv3D(256, (3, 3, 3), strides=(1, 2, 2), padding=\"same\", activation=\"relu\")(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    \n",
    "    # Global pooling with explicit reshaping\n",
    "    x = tf.keras.layers.TimeDistributed(tf.keras.layers.GlobalAveragePooling2D())(x)\n",
    "    \n",
    "    # Ensure proper shape before GRU layer\n",
    "    x = tf.keras.layers.Reshape((-1, x.shape[-1]))(x)\n",
    "    \n",
    "    # Bidirectional GRU layer\n",
    "    x = Bidirectional(GRU(128, return_sequences=False, \n",
    "                         dropout=0.2, \n",
    "                         recurrent_dropout=0.2))(x)\n",
    "    \n",
    "    # Dense layers\n",
    "    x = Dropout(0.4)(x)\n",
    "    x = Dense(64, activation=\"relu\")(x)\n",
    "    x = Dense(32, activation=\"relu\")(x)\n",
    "    \n",
    "    # Output layer\n",
    "    output = Dense(len(class_vocab), activation=\"softmax\", name='classification_output')(x)\n",
    "    \n",
    "    # Create and compile model\n",
    "    model = Model(inputs=video_input, outputs=output, name=\"inflated_3d_model\")\n",
    "    \n",
    "    # Learning rate schedule\n",
    "    lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "        initial_learning_rate=0.01,\n",
    "        decay_steps=10000,\n",
    "        decay_rate=0.9\n",
    "    )\n",
    "    \n",
    "    # Optimizer\n",
    "    adam_optimizer = tf.keras.optimizers.AdamW(learning_rate=lr_schedule)\n",
    "    \n",
    "    # Compile with proper loss and metrics\n",
    "    model.compile(\n",
    "        loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
    "        optimizer=adam_optimizer,\n",
    "        metrics=[\"accuracy\"],\n",
    "    )\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"inflated_3d_model\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"inflated_3d_model\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ video_input (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)  │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv3d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv3D</span>)                 │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">112</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">112</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>) │        <span style=\"color: #00af00; text-decoration-color: #00af00\">28,288</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization             │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">112</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">112</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>) │           <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv3d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv3D</span>)               │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)  │       <span style=\"color: #00af00; text-decoration-color: #00af00\">221,312</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_1           │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)  │           <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv3d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv3D</span>)               │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)  │       <span style=\"color: #00af00; text-decoration-color: #00af00\">884,992</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_2           │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)  │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ time_distributed                │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)          │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TimeDistributed</span>)               │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ reshape (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Reshape</span>)               │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)          │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ bidirectional (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>)   │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)              │       <span style=\"color: #00af00; text-decoration-color: #00af00\">296,448</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)              │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)               │        <span style=\"color: #00af00; text-decoration-color: #00af00\">16,448</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)               │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,080</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ classification_output (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)   │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)               │           <span style=\"color: #00af00; text-decoration-color: #00af00\">330</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ video_input (\u001b[38;5;33mInputLayer\u001b[0m)        │ (\u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m20\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m3\u001b[0m)  │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv3d (\u001b[38;5;33mConv3D\u001b[0m)                 │ (\u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m20\u001b[0m, \u001b[38;5;34m112\u001b[0m, \u001b[38;5;34m112\u001b[0m, \u001b[38;5;34m64\u001b[0m) │        \u001b[38;5;34m28,288\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization             │ (\u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m20\u001b[0m, \u001b[38;5;34m112\u001b[0m, \u001b[38;5;34m112\u001b[0m, \u001b[38;5;34m64\u001b[0m) │           \u001b[38;5;34m256\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv3d_1 (\u001b[38;5;33mConv3D\u001b[0m)               │ (\u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m20\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m128\u001b[0m)  │       \u001b[38;5;34m221,312\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_1           │ (\u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m20\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m128\u001b[0m)  │           \u001b[38;5;34m512\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv3d_2 (\u001b[38;5;33mConv3D\u001b[0m)               │ (\u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m20\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m256\u001b[0m)  │       \u001b[38;5;34m884,992\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_2           │ (\u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m20\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m256\u001b[0m)  │         \u001b[38;5;34m1,024\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ time_distributed                │ (\u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m20\u001b[0m, \u001b[38;5;34m256\u001b[0m)          │             \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mTimeDistributed\u001b[0m)               │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ reshape (\u001b[38;5;33mReshape\u001b[0m)               │ (\u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m20\u001b[0m, \u001b[38;5;34m256\u001b[0m)          │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ bidirectional (\u001b[38;5;33mBidirectional\u001b[0m)   │ (\u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m256\u001b[0m)              │       \u001b[38;5;34m296,448\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m256\u001b[0m)              │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m64\u001b[0m)               │        \u001b[38;5;34m16,448\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m)               │         \u001b[38;5;34m2,080\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ classification_output (\u001b[38;5;33mDense\u001b[0m)   │ (\u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m10\u001b[0m)               │           \u001b[38;5;34m330\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,451,690</span> (5.54 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m1,451,690\u001b[0m (5.54 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,450,794</span> (5.53 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,450,794\u001b[0m (5.53 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">896</span> (3.50 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m896\u001b[0m (3.50 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = build_inflated_3d_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_new_model_version():\n",
    "    model_version = 1\n",
    "    while os.path.exists(f\"{MODEL_BASE_PATH}/v_{model_version}\"):\n",
    "        model_version += 1\n",
    "    return model_version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_version_path():\n",
    "    model_version = get_new_model_version()\n",
    "    model_version_path = f\"{MODEL_BASE_PATH}/v_{model_version}\"\n",
    "\n",
    "    return model_version_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_path(model_version_path):\n",
    "    model_path = os.path.join(model_version_path, f\"{MODEL_NAME}.keras\")\n",
    "    \n",
    "    return model_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utility to run the experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, model_path):\n",
    "    callbacks = [\n",
    "        ModelCheckpoint(\n",
    "            model_path,\n",
    "            save_weights_only=False,\n",
    "            save_best_only=True,\n",
    "            monitor=\"val_accuracy\",\n",
    "            verbose=1,\n",
    "        ),\n",
    "        EarlyStopping(monitor=\"val_loss\", patience=10, restore_best_weights=True),\n",
    "        tf.keras.callbacks.ReduceLROnPlateau(factor=0.2, patience=3),\n",
    "    ]\n",
    "\n",
    "    train_dataset, val_dataset = prepare_train_val_datasets(\n",
    "        train_df, val_df, BATCH_SIZE\n",
    "    )\n",
    "\n",
    "    history = model.fit(\n",
    "        train_dataset,\n",
    "        validation_data=val_dataset,\n",
    "        steps_per_epoch=len(train_df) // BATCH_SIZE,\n",
    "        validation_steps=len(val_df) // BATCH_SIZE,\n",
    "        epochs=EPOCHS,\n",
    "        callbacks=callbacks,\n",
    "        verbose=1,\n",
    "    )\n",
    "\n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/60\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "as_list() is not defined on an unknown TensorShape.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m model_version_path \u001b[38;5;241m=\u001b[39m get_model_version_path()\n\u001b[1;32m      2\u001b[0m model_path \u001b[38;5;241m=\u001b[39m get_model_path(model_version_path)\n\u001b[0;32m----> 4\u001b[0m i3d_model, history \u001b[38;5;241m=\u001b[39m train_model(model, model_path)\n",
      "Cell \u001b[0;32mIn[23], line 18\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, model_path)\u001b[0m\n\u001b[1;32m      2\u001b[0m callbacks \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m      3\u001b[0m     ModelCheckpoint(\n\u001b[1;32m      4\u001b[0m         model_path,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     11\u001b[0m     tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mcallbacks\u001b[38;5;241m.\u001b[39mReduceLROnPlateau(factor\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m, patience\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m),\n\u001b[1;32m     12\u001b[0m ]\n\u001b[1;32m     14\u001b[0m train_dataset, val_dataset \u001b[38;5;241m=\u001b[39m prepare_train_val_datasets(\n\u001b[1;32m     15\u001b[0m     train_df, val_df, BATCH_SIZE\n\u001b[1;32m     16\u001b[0m )\n\u001b[0;32m---> 18\u001b[0m history \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mfit(\n\u001b[1;32m     19\u001b[0m     train_dataset,\n\u001b[1;32m     20\u001b[0m     validation_data\u001b[38;5;241m=\u001b[39mval_dataset,\n\u001b[1;32m     21\u001b[0m     steps_per_epoch\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(train_df) \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m BATCH_SIZE,\n\u001b[1;32m     22\u001b[0m     validation_steps\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(val_df) \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m BATCH_SIZE,\n\u001b[1;32m     23\u001b[0m     epochs\u001b[38;5;241m=\u001b[39mEPOCHS,\n\u001b[1;32m     24\u001b[0m     callbacks\u001b[38;5;241m=\u001b[39mcallbacks,\n\u001b[1;32m     25\u001b[0m     verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m     26\u001b[0m )\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model, history\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "\u001b[0;31mValueError\u001b[0m: as_list() is not defined on an unknown TensorShape."
     ]
    }
   ],
   "source": [
    "model_version_path = get_model_version_path()\n",
    "model_path = get_model_path(model_version_path)\n",
    "\n",
    "i3d_model, history = train_model(model, model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(model_path):\n",
    "    print(f\"Model saved at: {model_path}\")\n",
    "    model_size = os.path.getsize(model_path) / (1024 * 1024)\n",
    "    print(f\"Model size: {model_size:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualise the training and validation loss and accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_training_metrics(history, metrics_path):\n",
    "    train_loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "    train_accuracy = history.history['accuracy']\n",
    "    val_accuracy = history.history['val_accuracy']\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    # Loss plot\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(train_loss, label='Training Loss')\n",
    "    plt.plot(val_loss, label='Validation Loss')\n",
    "    plt.title(\"Training and Validation Loss\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()\n",
    "\n",
    "    # Accuracy plot\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(train_accuracy, label='Training Accuracy')\n",
    "    plt.plot(val_accuracy, label='Validation Accuracy')\n",
    "    plt.title(\"Training and Validation Accuracy\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.savefig(metrics_path)\n",
    "    print(f\"Training metrics plot saved at: {metrics_path}\")\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_image_path(metric_type, model_version_path):\n",
    "    image_path = f\"{model_version_path}/{metric_type}.png\"\n",
    "    return image_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = get_model_image_path(\"loss\", model_version_path)\n",
    "visualize_training_metrics(history, image_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation and Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate on the entire test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_gen = data_generator(test_df, batch_size=32)\n",
    "\n",
    "i3d_model.load_weights(model_path) # Load the best weights\n",
    "test_loss, test_accuracy = i3d_model.evaluate(test_gen, steps=len(test_df) // 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Test Loss: {test_loss :.4f}\")\n",
    "print(f\"Test Accuracy: {test_accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation with single sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make predictions on a single video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, video_frames, true_label):\n",
    "    # Expand the dimensions to match model input shape\n",
    "    video_frames = np.expand_dims(\n",
    "        video_frames, axis=0\n",
    "    )  # Shape (1, num_frames, height, width, channels)\n",
    "\n",
    "    # Get model predictions\n",
    "    y_pred = model.predict(video_frames)\n",
    "\n",
    "    # Get the predicted label (index of the highest probability)\n",
    "    predicted_label_index = np.argmax(y_pred, axis=1)[0]\n",
    "    predicted_label = class_vocab[predicted_label_index]  # Map index to label\n",
    "\n",
    "    print(f\"Prediction probabilities: {y_pred}\")\n",
    "    print(f\"Predicted label index: {predicted_label_index}\")\n",
    "\n",
    "    return true_label, y_pred, predicted_label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display predicted image as GIF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_as_gif(frames, model_version_path, save=False):\n",
    "    gif_path = f\"{model_version_path}/test_animation.gif\"\n",
    "    converted_images = frames.astype(np.uint8)\n",
    "\n",
    "    if save:\n",
    "        imageio.mimsave(gif_path, converted_images, duration=100)\n",
    "        print(f\"GIF saved at {gif_path}\")\n",
    "\n",
    "    return Image.open(gif_path)  # Display the gif"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choosing a random video to use for prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_test_video(test_df, test_labels, save_gif=False):\n",
    "    # Select a random test video\n",
    "    random_index = np.random.randint(len(test_df))\n",
    "    \n",
    "    # Get the test video path\n",
    "    test_video = test_df[\"video_path\"].values[random_index]\n",
    "\n",
    "    # Get the true label of the test video\n",
    "    true_label_index = test_labels.tolist()[random_index][0]\n",
    "    true_label = class_vocab[true_label_index]\n",
    "\n",
    "    # Load video frames\n",
    "    test_video_frames = load_video_frames(test_video)\n",
    "\n",
    "    print(f\"Test video path: {test_video}\")\n",
    "    print(f\"Label: {true_label}\")\n",
    "    \n",
    "    # Display the shape of the test video frames\n",
    "    print(f\"\"\"\n",
    "    Test video frames shape:\n",
    "      - {test_video_frames.shape[0]} frames\n",
    "      - {test_video_frames.shape[1]} pixels (height) x {test_video_frames.shape[2]} pixels (width)\n",
    "      - {test_video_frames.shape[3]} color channels\n",
    "    \"\"\")\n",
    "\n",
    "    display_as_gif(test_video_frames[:MAX_SEQ_LENGTH], model_version_path, save_gif)\n",
    "\n",
    "    return test_video_frames, true_label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the prediction on the test video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_labels = encode_labels(test_df[\"label\"].values)\n",
    "\n",
    "test_video_frames, true_label = get_test_video(test_df, test_labels, save_gif=True)\n",
    "y_true, y_pred, predicted_label = predict(i3d_model, test_video_frames, true_label)\n",
    "\n",
    "print(f\"\\nTrue label: {true_label}\")\n",
    "print(f\"Predicted label: {predicted_label}\")\n",
    "\n",
    "print(f\"y_true: {y_true}\")\n",
    "print(f\"y_pred: {y_pred}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_test_videos(model, test_gen, batch_size=32):\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "\n",
    "    # Calculate the number of steps per epoch\n",
    "    steps = len(test_gen) // batch_size\n",
    "\n",
    "    for i in range(steps):\n",
    "        # Get a batch of frames and labels\n",
    "        batch_frames, batch_labels = next(test_gen)\n",
    "\n",
    "        # Predict using the model\n",
    "        predictions = model.predict(batch_frames, batch_size=batch_size)\n",
    "\n",
    "        # Get the predicted class labels\n",
    "        batch_pred_labels = np.argmax(predictions, axis=1)\n",
    "\n",
    "        # Append the true labels and predicted labels\n",
    "        # Convert one-hot labels to integers\n",
    "        y_true.extend(np.argmax(batch_labels, axis=1))\n",
    "        y_pred.extend(batch_pred_labels)\n",
    "\n",
    "        # Print progress every 100 batches\n",
    "        if i % 50 == 0:\n",
    "            print(f\"Processed {i+1}/{steps} batches\")\n",
    "\n",
    "    return np.array(y_true), np.array(y_pred), predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utility to save classification metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_classification_report(report, save_path):\n",
    "    report_data = []\n",
    "    lines = report.split(\"\\n\")\n",
    "\n",
    "    for line in filter(None, lines[2:-3]):  # Remove empty lines and headers/footers\n",
    "        row = line.split()\n",
    "        name = row[0]\n",
    "        stats = row[1:]\n",
    "\n",
    "        # Convert stats to float, handling support as int\n",
    "        stats = [float(val) for val in stats[:-1]] + [int(stats[-1])]\n",
    "        report_data.append([name] + stats)\n",
    "\n",
    "    report_df = pd.DataFrame(\n",
    "        report_data, columns=[\"Class\", \"Precision\", \"Recall\", \"F1-score\", \"Support\"]\n",
    "    )\n",
    "\n",
    "    report_df.to_csv(save_path, index=False)\n",
    "\n",
    "    print(f\"Classification report saved at: {save_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utility to display evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_evaluation_metrics(y_true, y_pred, predictions, target_classes):\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    print(f\"Top-1 Accuracy: {accuracy * 100:.2f}%\")\n",
    "\n",
    "    k = 2\n",
    "    top_k_acc = top_k_accuracy_score(y_true, predictions, k=k)\n",
    "    print(f\"Top-{k} Accuracy: {top_k_acc * 100:.2f}%\")\n",
    "\n",
    "    report = classification_report(y_true, y_pred, target_names=target_classes)\n",
    "    print(\"\\nClassification Report:\\n\", report)\n",
    "\n",
    "    save_classification_report(report, f'{model_version_path}/classification_metrics.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true, y_pred, predictions = predict_test_videos(i3d_model, test_gen)\n",
    "\n",
    "# y_pred = np.argmax(predictions, axis=1)\n",
    "\n",
    "# Print the results\n",
    "print(f\"True Labels: {y_true}\")\n",
    "print(f\"Predicted Labels: {y_pred}\")\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = np.sum(y_true == y_pred) / len(y_true)\n",
    "print(f\"Test Accuracy: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_evaluation_metrics(y_true, y_pred, predictions, class_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_confusion_matrix(y_true, y_pred, target_classes, plot_path, show_plot=False):\n",
    "    conf_matrix = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(\n",
    "        conf_matrix,\n",
    "        fmt=\"d\",\n",
    "        annot=True,\n",
    "        cmap=\"Blues\",\n",
    "        cbar=True,\n",
    "        xticklabels=target_classes,\n",
    "        yticklabels=target_classes,\n",
    "    )\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"True\")\n",
    "    plt.title(\"Confusion Matrix\")\n",
    "\n",
    "    if show_plot:\n",
    "        plt.show()    \n",
    "    else:\n",
    "        print(conf_matrix)\n",
    "\n",
    "    plt.savefig(plot_path)\n",
    "    print(f\"\\nConfusion matrix saved at: {plot_path}\")\n",
    "\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_labels = np.array(test_labels).flatten()\n",
    "\n",
    "cm_plot_path = get_model_image_path(\"confusion_matrix\", model_version_path)\n",
    "display_confusion_matrix(true_labels, y_pred, class_vocab, cm_plot_path, show_plot=False)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 4915922,
     "datasetId": 2807884,
     "sourceId": 4849320,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30787,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
