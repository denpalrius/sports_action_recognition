{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sports Action Recognition Using CNN-RNN Architecture with Transfer Learning on `UCF101 10 Sports actions`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-06T02:10:35.139379Z",
     "iopub.status.busy": "2024-11-06T02:10:35.138805Z",
     "iopub.status.idle": "2024-11-06T02:10:51.356809Z",
     "shell.execute_reply": "2024-11-06T02:10:51.355557Z",
     "shell.execute_reply.started": "2024-11-06T02:10:35.139326Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "! pip install kagglehub -Uq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-11-06T02:10:51.359159Z",
     "iopub.status.busy": "2024-11-06T02:10:51.358754Z",
     "iopub.status.idle": "2024-11-06T02:11:09.254053Z",
     "shell.execute_reply": "2024-11-06T02:11:09.253134Z",
     "shell.execute_reply.started": "2024-11-06T02:10:51.359087Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import accuracy_score, top_k_accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "import imageio\n",
    "import cv2\n",
    "from IPython.display import Image\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from keras.metrics import top_k_categorical_accuracy\n",
    "\n",
    "import pprint\n",
    "from tqdm import tqdm\n",
    "\n",
    "import kagglehub\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download latest version of the ucf101-action-recognition dataset from Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = kagglehub.dataset_download(\"matthewjansen/ucf101-action-recognition\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-06T02:12:09.427435Z",
     "iopub.status.busy": "2024-11-06T02:12:09.427055Z",
     "iopub.status.idle": "2024-11-06T02:12:09.431825Z",
     "shell.execute_reply": "2024-11-06T02:12:09.430893Z",
     "shell.execute_reply.started": "2024-11-06T02:12:09.427384Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(\"Path to dataset files: \\n\", path)\n",
    "print(\"\\nFiles in dataset directory:\\n\", os.listdir(path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Class Definition**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sports_actions = [\n",
    "    \"SkyDiving\",\n",
    "    \"Biking\",\n",
    "    \"HorseRace\",\n",
    "    \"Surfing\",\n",
    "    \"TennisSwing\",\n",
    "    \"Punch\",\n",
    "    \"Basketball\",\n",
    "    \"JumpRope\",\n",
    "    \"Archery\",\n",
    "    \"Skiing\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utility to transform video paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(dataset_type):\n",
    "    dataset_path = os.path.join(path, f\"{dataset_type}.csv\")\n",
    "    dataset = pd.read_csv(dataset_path)\n",
    "\n",
    "    # Filter dataset to only include the specified sports actions\n",
    "    filtered_dataset = dataset[dataset[\"label\"].isin(sports_actions)]\n",
    "\n",
    "    return pd.DataFrame(\n",
    "        {\n",
    "            \"label\": filtered_dataset[\"label\"],\n",
    "            \"video_name\": filtered_dataset[\"clip_name\"],\n",
    "            \"rel_path\": filtered_dataset[\"clip_path\"],\n",
    "            \"video_path\": filtered_dataset[\"clip_path\"].apply(lambda x: f\"{path}{x}\"),\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the train and test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = load_dataset(\"train\")\n",
    "val_df = load_dataset(\"val\")\n",
    "test_df = load_dataset(\"test\")\n",
    "\n",
    "print(f\"Total videos for training: {len(train_df)}\")\n",
    "print(f\"Total videos for validation: {len(val_df)}\")\n",
    "print(f\"Total videos for testing: {len(test_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of unique classes in training set: \", len(train_df[\"label\"].unique()))\n",
    "print(\"Number of unique classes in validation set: \", len(val_df[\"label\"].unique()))\n",
    "print(\"Number of unique classes in test set: \", len(test_df[\"label\"].unique()))\n",
    "\n",
    "print(\"\\nLabels: \\n\", train_df[\"label\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configs & Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-06T02:12:09.480471Z",
     "iopub.status.busy": "2024-11-06T02:12:09.480187Z",
     "iopub.status.idle": "2024-11-06T02:12:09.484927Z",
     "shell.execute_reply": "2024-11-06T02:12:09.484040Z",
     "shell.execute_reply.started": "2024-11-06T02:12:09.480440Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "MODEL_NAME = \"cnn_rnn_ucf101_10c_tl_v9_200f_InceptionResNetV2\"\n",
    "MODEL_BASE_PATH = f\"../models/{MODEL_NAME}\"\n",
    "DATA_FILE_BASE = \"../data/preprocessed_200f_InceptionResNetV2\"\n",
    "\n",
    "IMG_SIZE = 224\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 120\n",
    "LEARNING_RATE = 1e-4\n",
    "\n",
    "MAX_SEQ_LENGTH = 200 # Chosen according to distribution of video lengths below\n",
    "NUM_FEATURES = 2048"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Video Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Label Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encode the class labels as integers using the Keras StringLookup layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(train_df[\"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_processor = keras.layers.StringLookup(num_oov_indices=0, vocabulary=sports_actions)\n",
    "\n",
    "class_vocab = label_processor.get_vocabulary()\n",
    "\n",
    "print(f\"Vocabulary: {class_vocab}\")\n",
    "print(f\"Number of classes: {len(class_vocab)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utility to convert string labels to one-hot encoded format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_labels(labels: np.ndarray) -> np.ndarray:\n",
    "    integer_labels = keras.ops.convert_to_numpy(label_processor(labels[..., None]))\n",
    "    return keras.utils.to_categorical(integer_labels, num_classes=len(class_vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Review video category distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def review_class_distribution(dataset, dataset_name):\n",
    "    class_counts = dataset[\"label\"].value_counts()\n",
    "    return class_counts\n",
    "\n",
    "# Get class distributions for each dataset\n",
    "train_class_counts = review_class_distribution(train_df, \"Train\")\n",
    "val_class_counts = review_class_distribution(val_df, \"Validation\")\n",
    "test_class_counts = review_class_distribution(test_df, \"Test\")\n",
    "\n",
    "# Create DataFrame for distribution and calculate average\n",
    "distribution_df = pd.DataFrame({\n",
    "    \"Train\": train_class_counts,\n",
    "    \"Validation\": val_class_counts,\n",
    "    \"Test\": test_class_counts\n",
    "}).fillna(0)\n",
    "\n",
    "distribution_df[\"Average\"] = distribution_df.mean(axis=1).round().astype(int)\n",
    "print(\"Combined average number of videos per class:\")\n",
    "print(distribution_df)\n",
    "\n",
    "# Plot the distribution\n",
    "plot_distribution_df = distribution_df.drop(columns=\"Average\")\n",
    "plot_distribution_df.plot(kind=\"bar\", figsize=(10, 5))\n",
    "plt.title(\"Class Distribution Comparison Across Train, Validation, and Test Sets\")\n",
    "plt.xlabel(\"Class Labels\")\n",
    "plt.ylabel(\"Number of Videos\")\n",
    "plt.legend(title=\"Dataset\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Review Video frame distribution "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Count the number of frames for each video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_frames_per_video(video_paths):\n",
    "    frame_counts = []\n",
    "\n",
    "    for video_path in video_paths: \n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        count = 0\n",
    "        \n",
    "        while cap.isOpened():\n",
    "            ret, _ = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            count += 1\n",
    "        cap.release()\n",
    "        frame_counts.append(count)\n",
    "\n",
    "    return frame_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_frame_distribution(frame_counts):\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    sns.violinplot(x=frame_counts)\n",
    "    plt.title(\"Violin Plot of Frame Counts per Video\")\n",
    "    plt.xlabel(\"Number of Frames\")\n",
    "    plt.xlabel(\"Number of Frames\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_counts = count_frames_per_video(train_df[\"video_path\"].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Distribution of the frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Minimum number of frames per video: {np.min(frame_counts)}\")\n",
    "print(f\"Maximum number of frames per video: {np.max(frame_counts)}\")\n",
    "print(f\"Average number of frames per video: {np.mean(frame_counts):.2f}\")\n",
    "print(f\"Median number of frames per video: {np.median(frame_counts)}\")\n",
    "print(f\"Standard deviation of frames per video: {np.std(frame_counts):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_frame_distribution(frame_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Video preprocessing utilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Function to resize the video frames to a square shape without distorting their content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-06T02:12:09.486337Z",
     "iopub.status.busy": "2024-11-06T02:12:09.486016Z",
     "iopub.status.idle": "2024-11-06T02:12:09.496479Z",
     "shell.execute_reply": "2024-11-06T02:12:09.495613Z",
     "shell.execute_reply.started": "2024-11-06T02:12:09.486305Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def crop_center_square(frame):\n",
    "    y, x = frame.shape[0:2]  # Get the height (y) and width (x) of the image\n",
    "    min_dim = min(y, x)       # Find the smallest dimension (either height or width)\n",
    "    start_x = (x // 2) - (min_dim // 2)  # Calculate the horizontal starting point for the crop\n",
    "    start_y = (y // 2) - (min_dim // 2)  # Calculate the vertical starting point for the crop\n",
    "\n",
    "    return frame[start_y : start_y + min_dim, start_x : start_x + min_dim]  # Return the cropped square"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Crop,resize, and reorder color channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-06T02:12:09.499763Z",
     "iopub.status.busy": "2024-11-06T02:12:09.499469Z",
     "iopub.status.idle": "2024-11-06T02:12:09.507023Z",
     "shell.execute_reply": "2024-11-06T02:12:09.506339Z",
     "shell.execute_reply.started": "2024-11-06T02:12:09.499731Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def load_and_preprocess_video(video_path, max_frames=0, resize=(IMG_SIZE, IMG_SIZE)):    \n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    frames = []\n",
    "    try:\n",
    "        while True:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            frame = crop_center_square(frame) # Crop center square\n",
    "            frame = cv2.resize(frame, resize) # Resize the image (In this case to 224x224)\n",
    "            frame = frame[:, :, [2, 1, 0]] # Reorder the color channels from OpenCV BGR to RGB\n",
    "            frame = frame / 255.0\n",
    "            \n",
    "            frames.append(frame)\n",
    "\n",
    "            if len(frames) == max_frames:\n",
    "                break\n",
    "    finally:\n",
    "        cap.release()\n",
    "   \n",
    "    return np.array(frames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Extraction with Transfer Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using a pre-trained network to extract meaningful features from the extracted frames\n",
    "- The Inception-ResNet V2 model without the top layer\n",
    "- Pretrained on ImageNet-1k dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-06T02:12:09.510210Z",
     "iopub.status.busy": "2024-11-06T02:12:09.507987Z",
     "iopub.status.idle": "2024-11-06T02:12:17.591303Z",
     "shell.execute_reply": "2024-11-06T02:12:17.590307Z",
     "shell.execute_reply.started": "2024-11-06T02:12:09.510168Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def build_feature_extractor():\n",
    "    feature_extractor = keras.applications.InceptionResNetV2(\n",
    "        weights=\"imagenet\",\n",
    "        include_top=False,\n",
    "        pooling=\"avg\",  # Global average pooling to reduce the output to a vector\n",
    "        input_shape=(IMG_SIZE, IMG_SIZE, 3),\n",
    "    )\n",
    "\n",
    "    inputs = keras.Input((IMG_SIZE, IMG_SIZE, 3))\n",
    "    # Preprocess the input using Inception-ResNet V2 preprocessing function\n",
    "    preprocessed = keras.applications.inception_resnet_v2.preprocess_input(inputs)\n",
    "    \n",
    "    # Extract features\n",
    "    features = feature_extractor(preprocessed)\n",
    "    \n",
    "    return keras.Model(inputs, features, name=\"feature_extractor\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "feature_extractor = build_feature_extractor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper function to create masks and features for a single video\n",
    "- `frame_features` will contain extracted features per frame\n",
    "- `frame_masks` will contain booleans denoting if a timestep/frame is padded or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_video_features_and_mask(frames):\n",
    "    mask = np.zeros((MAX_SEQ_LENGTH,), dtype=\"bool\")\n",
    "    features = np.zeros((MAX_SEQ_LENGTH, NUM_FEATURES), dtype=\"float32\")\n",
    "\n",
    "    # Extract features for each frame\n",
    "    for i, frame in enumerate(frames[:MAX_SEQ_LENGTH]):\n",
    "        feature = feature_extractor(tf.expand_dims(frame, 0))\n",
    "        features[i] = feature\n",
    "        mask[i] = True  # Mark this frame as valid in the mask\n",
    "\n",
    "    return features, mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Video frames preprocessing and saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_process_batch(video_paths, dataset_type):\n",
    "    start_time = time.time()\n",
    "\n",
    "    features_list = []\n",
    "    masks_list = []\n",
    "\n",
    "    for video_path in video_paths:\n",
    "        frames = load_and_preprocess_video(video_path)\n",
    "        features, mask = create_video_features_and_mask(frames)\n",
    "\n",
    "        features_list.append(features)\n",
    "        masks_list.append(mask)\n",
    "\n",
    "    # Stack all features and masks for model input\n",
    "    features_batch = np.stack(features_list)\n",
    "    masks_batch = np.stack(masks_list)\n",
    "\n",
    "    elapsed = time.time() - start_time\n",
    "    print(f\"Processed {len(video_paths)} {dataset_type} videos in {elapsed:.2f}s\")\n",
    "\n",
    "    return features_batch, masks_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocess entire dataset and save to disk for faster future loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def preprocess_dataset_with_resume(data_df, save_path, dataset_type):\n",
    "#     # Process in smaller chunks to manage memory\n",
    "#     CHUNK_SIZE = 64\n",
    "#     total_length = len(data_df)\n",
    "#     save_path.replace(\".npy\", f\"_progress.npy\")\n",
    "#     processed_chunks_file = f\"{save_path.replace(\".npy\", f\"_progress.npy\")}\"\n",
    "    \n",
    "#     # Load progress\n",
    "#     if os.path.exists(processed_chunks_file):\n",
    "#         processed_chunks = np.load(processed_chunks_file).tolist()\n",
    "#     else:\n",
    "#         processed_chunks = []\n",
    "\n",
    "#     all_features = []\n",
    "#     all_masks = []\n",
    "#     all_labels = []\n",
    "    \n",
    "#     # Load already processed data if exists\n",
    "#     if os.path.exists(save_path):\n",
    "#         data = np.load(save_path)\n",
    "#         all_features = [data[\"features\"]]\n",
    "#         all_masks = [data[\"masks\"]]\n",
    "#         all_labels = [data[\"labels\"]]\n",
    "\n",
    "#     with tqdm(total=(total_length // CHUNK_SIZE) + 1, desc=\"Processing dataset\", colour=\"green\") as pbar:\n",
    "#         pbar.update(len(processed_chunks))  # Update progress bar based on already processed chunks\n",
    "        \n",
    "#         for i in range(0, total_length, CHUNK_SIZE):\n",
    "#             if i in processed_chunks:\n",
    "#                 continue  # Skip already processed chunks\n",
    "            \n",
    "#             chunk_df = data_df.iloc[i:i + CHUNK_SIZE]\n",
    "#             labels = keras.ops.convert_to_numpy(label_processor(chunk_df[\"label\"].values[..., None]))\n",
    "#             features, masks = load_and_process_batch(\n",
    "#                 video_paths=chunk_df[\"video_path\"].tolist(),\n",
    "#                 dataset_type=dataset_type\n",
    "#             )\n",
    "            \n",
    "#             all_features.append(features)\n",
    "#             all_masks.append(masks)\n",
    "#             all_labels.append(labels)\n",
    "            \n",
    "#             # Save the processed chunk indices\n",
    "#             processed_chunks.append(i)\n",
    "#             np.save(processed_chunks_file, processed_chunks)\n",
    "            \n",
    "#             # Save the current state of processed data\n",
    "#             save_features_masks_and_labels(\n",
    "#                 np.concatenate(all_features, axis=0),\n",
    "#                 np.concatenate(all_masks, axis=0),\n",
    "#                 np.concatenate(all_labels, axis=0),\n",
    "#                 save_path,\n",
    "#                 dataset_type\n",
    "#             )\n",
    "            \n",
    "#             pbar.update(1)  # Increment progress bar\n",
    "\n",
    "#     # Clean up progress file after completion\n",
    "#     if os.path.exists(processed_chunks_file):\n",
    "#         os.remove(processed_chunks_file)\n",
    "\n",
    "#     return (\n",
    "#         np.concatenate(all_features, axis=0),\n",
    "#         np.concatenate(all_masks, axis=0),\n",
    "#         np.concatenate(all_labels, axis=0),\n",
    "#     )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_dataset(data_df, dataset_type):\n",
    "    # Process in smaller chunks to manage memory\n",
    "    CHUNK_SIZE = 64\n",
    "    \n",
    "    all_features = []\n",
    "    all_masks = []\n",
    "    all_labels = []\n",
    "    \n",
    "    for i in range(0, len(data_df), CHUNK_SIZE):\n",
    "        chunk_df = data_df.iloc[i:i + CHUNK_SIZE]\n",
    "        # labels = encode_labels(chunk_df[\"label\"])\n",
    "        # labels = label_processor(chunk_df[\"label\"].values[..., None])\n",
    "        labels = keras.ops.convert_to_numpy(label_processor(chunk_df[\"label\"].values[..., None]))\n",
    "        features, masks = load_and_process_batch(\n",
    "            video_paths=chunk_df[\"video_path\"].tolist(),\n",
    "            dataset_type=dataset_type\n",
    "        )\n",
    "        \n",
    "        all_features.append(features)\n",
    "        all_masks.append(masks)\n",
    "        all_labels.append(labels)\n",
    "    \n",
    "    # Combine all chunks\n",
    "    features = np.concatenate(all_features, axis=0)\n",
    "    masks = np.concatenate(all_masks, axis=0)\n",
    "    labels = np.concatenate(all_labels, axis=0)\n",
    "    \n",
    "    return features, masks, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save features and mask to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_features_masks_and_labels(features, masks, labels, save_path, dataset_type):\n",
    "    np.savez_compressed(save_path, features=features, masks=masks, labels=labels)\n",
    "    print(f\"Saved {dataset_type} features, masks, and labels to {save_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the dataset from a saved file if it exists, otherwise process it, save it, and load it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_or_preprocess_dataset(data_df, save_path, dataset_type):\n",
    "    if os.path.exists(save_path):\n",
    "        print(f\"Loading dataset from {save_path}\")\n",
    "        data = np.load(save_path)\n",
    "        features, masks, labels = data[\"features\"], data[\"masks\"], data[\"labels\"]\n",
    "                    \n",
    "    else:\n",
    "        print(f\"Saved {dataset_type} dataset not found. Processing dataset...\")\n",
    "        features, masks, labels = preprocess_dataset(data_df, dataset_type)\n",
    "        save_features_masks_and_labels(features, masks, labels, save_path, dataset_type)\n",
    "        \n",
    "    return features, masks, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load datafiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_save_path = f\"{DATA_FILE_BASE}/train_features_masks_labels.npz\"\n",
    "val_save_path = f\"{DATA_FILE_BASE}/val_features_masks_labels.npz\"\n",
    "test_save_path = f\"{DATA_FILE_BASE}/test_features_masks_labels.npz\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract the frame features, feature masks and labels for the `train` set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features, train_masks, train_labels = load_or_preprocess_dataset(train_df, train_save_path, \"Training\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract the frame features, feature masks and labels for the `validation` set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_features, val_masks, val_labels = load_or_preprocess_dataset(val_df, val_save_path, \"Validation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract the frame features, feature masks and labels for the `test` set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_features, test_masks, test_labels = load_or_preprocess_dataset(test_df, test_save_path, \"Test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Review dataset size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"════════════════════ TRAIN SET ════════════════════\\n\")\n",
    "\n",
    "# Train set\n",
    "total_samples, frame_count, num_of_features = train_features.shape\n",
    "print(f\"\"\"Frame features in train set: {train_features.shape} \n",
    "    → {total_samples} samples\n",
    "    → {frame_count} frames per video\n",
    "    → {num_of_features} features per frame\n",
    "\"\"\")\n",
    "\n",
    "total_samples, mask_count = train_masks.shape\n",
    "print(f\"\"\"Frame masks in train set: {train_masks.shape} \n",
    "    → {total_samples} samples\n",
    "    → {mask_count} masks per video\n",
    "\"\"\")\n",
    "print(f\"Labels in train set: {train_labels.shape} → {train_labels.shape[0]} samples\\n\")\n",
    "\n",
    "print(\"════════════════════ VALIDATION SET ════════════════════\\n\")\n",
    "\n",
    "# Validation set\n",
    "print(f\"Frame features in validation set: {val_features.shape}\")\n",
    "print(f\"Frame masks in validation set: {val_masks.shape}\")\n",
    "print(f\"Labels in validation set: {val_labels.shape} → {val_labels.shape[0]} samples\\n\")\n",
    "\n",
    "print(\"════════════════════ TEST SET ════════════════════\\n\")\n",
    "\n",
    "# Test set\n",
    "print(f\"Frame features in test set: {test_features.shape}\")\n",
    "print(f\"Frame masks in test set: {test_masks.shape}\")\n",
    "print(f\"Labels in test set: {test_labels.shape} → {test_labels.shape[0]} samples\")\n",
    "\n",
    "# Check the size consistency\n",
    "assert train_features.shape[0] == train_masks.shape[0] == train_labels.shape[0], \"Inconsistent train dataset sizes\"\n",
    "assert val_features.shape[0] == val_masks.shape[0] == val_labels.shape[0], \"Inconsistent validation dataset sizes\"\n",
    "assert test_features.shape[0] == test_masks.shape[0] == test_labels.shape[0], \"Inconsistent test dataset sizes\"\n",
    "print(\"\\nAll dataset sizes are consistent.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Tensorflow datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tf_dataset(features, masks, labels, batch_size=32, dataset_type=None):\n",
    "    features_tensor = tf.convert_to_tensor(features, dtype=tf.float32)\n",
    "    masks_tensor = tf.convert_to_tensor(masks, dtype=tf.bool)\n",
    "    labels_tensor = tf.convert_to_tensor(labels, dtype=tf.float32)\n",
    "\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(((features_tensor, masks_tensor), labels_tensor))\n",
    "\n",
    "    # Shuffle and batch the dataset\n",
    "    if dataset_type == \"Training\":\n",
    "        dataset = dataset.shuffle(buffer_size=len(features))\n",
    "    dataset = dataset.batch(batch_size)\n",
    "\n",
    "    # Prefetch for performance optimization\n",
    "    dataset = dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = create_tf_dataset(train_features, train_masks, train_labels, batch_size=32, dataset_type=\"Training\")\n",
    "val_dataset = create_tf_dataset(val_features, val_masks, val_labels, batch_size=32, dataset_type=\"Validation\")\n",
    "test_dataset = create_tf_dataset(test_features, test_masks, test_labels, batch_size=32, dataset_type=\"Test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN GRU Model Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recurrent Neural Network architecture with GRU (Gated Recurrent Unit) layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_rnn_sequence_model():\n",
    "    frame_features_input = keras.Input((MAX_SEQ_LENGTH, NUM_FEATURES))\n",
    "    mask_input = keras.Input((MAX_SEQ_LENGTH,), dtype=\"bool\")\n",
    "\n",
    "    x = keras.layers.GRU(16, return_sequences=True)(\n",
    "        frame_features_input, mask=mask_input\n",
    "    )\n",
    "    x = keras.layers.GRU(8)(x)\n",
    "    x = keras.layers.Dropout(0.4)(x)\n",
    "    x = keras.layers.Dense(64, activation=\"relu\")(x)\n",
    "    x = keras.layers.Dense(32, activation=\"relu\")(x)\n",
    "    output = keras.layers.Dense(len(class_vocab), activation=\"softmax\")(x)\n",
    "\n",
    "    rnn_model = keras.Model([frame_features_input, mask_input], output, name=\"rnn_sequence_model\")\n",
    "\n",
    "    adam_optimizer = keras.optimizers.Adam(learning_rate=LEARNING_RATE)\n",
    "    rnn_model.compile(\n",
    "        loss=\"sparse_categorical_crossentropy\",\n",
    "        optimizer=adam_optimizer,\n",
    "        metrics=[\"accuracy\"],\n",
    "    )\n",
    "\n",
    "    return rnn_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_seq_model = build_rnn_sequence_model()\n",
    "rnn_seq_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_new_model_version():\n",
    "    model_version = 1\n",
    "    while os.path.exists(f\"{MODEL_BASE_PATH}/v_{model_version}\"):\n",
    "        model_version += 1\n",
    "    return model_version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_version_path():\n",
    "    model_version = get_new_model_version()\n",
    "    model_version_path = f\"{MODEL_BASE_PATH}/v_{model_version}\"\n",
    "\n",
    "    return model_version_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_path(model_version_path):\n",
    "    model_path = os.path.join(model_version_path, f\"{MODEL_NAME}.keras\")\n",
    "    \n",
    "    return model_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utility to run the experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, model_path):\n",
    "    checkpoint = ModelCheckpoint(\n",
    "        model_path,\n",
    "        monitor=\"val_loss\",\n",
    "        mode=\"min\",  # Save the model when the loss decreases (when model improves)\n",
    "        save_weights_only=False,\n",
    "        save_best_only=True,\n",
    "        verbose=1,\n",
    "    )\n",
    "\n",
    "    history = model.fit(\n",
    "        train_dataset,\n",
    "        validation_data=val_dataset,\n",
    "        epochs=EPOCHS,\n",
    "        callbacks=[checkpoint],\n",
    "    )\n",
    "\n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model_version_path = get_model_version_path()\n",
    "model_path = get_model_path(model_version_path)\n",
    "\n",
    "sequence_model, history = train_model(rnn_seq_model, model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(model_path):\n",
    "    print(f\"Model saved at: {model_path}\")\n",
    "    model_size = os.path.getsize(model_path) / (1024 * 1024)\n",
    "    print(f\"Model size: {model_size:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualise the training and validation loss and accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_training_metrics(history, metrics_path):\n",
    "    train_loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "    train_accuracy = history.history['accuracy']\n",
    "    val_accuracy = history.history['val_accuracy']\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    # Loss plot\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(train_loss, label='Training Loss')\n",
    "    plt.plot(val_loss, label='Validation Loss')\n",
    "    plt.title(\"Training and Validation Loss\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()\n",
    "\n",
    "    # Accuracy plot\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(train_accuracy, label='Training Accuracy')\n",
    "    plt.plot(val_accuracy, label='Validation Accuracy')\n",
    "    plt.title(\"Training and Validation Accuracy\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    plt.savefig(metrics_path)\n",
    "    print(f\"Training metrics plot saved at: {metrics_path}\")\n",
    "\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_image_path(metric_type, model_version_path):\n",
    "    image_path = f\"{model_version_path}/{metric_type}.png\"\n",
    "    return image_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = get_model_image_path(\"loss\", model_version_path)\n",
    "visualize_training_metrics(history, image_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation and Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate on the entire test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_model.load_weights(model_path) # Load the best weights\n",
    "loss, accuracy = sequence_model.evaluate(test_dataset, batch_size=32)\n",
    "\n",
    "print(f\"Test Loss: {loss :.4f}\")\n",
    "print(f\"Traing Test Accuracy: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation with single sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to prepare the test video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_single_video(frames):\n",
    "    frames = frames[None, ...]\n",
    "    frame_mask = np.zeros(shape=(1, MAX_SEQ_LENGTH, ),dtype=\"bool\" )\n",
    "    frame_features = np.zeros(shape=(1, MAX_SEQ_LENGTH, NUM_FEATURES), dtype=\"float32\")\n",
    "\n",
    "    for i, frames_batch in enumerate(frames):\n",
    "        video_length = frames_batch.shape[0]\n",
    "        length = min(MAX_SEQ_LENGTH, video_length)\n",
    "        for j in range(length):\n",
    "            frame_features[i, j, :] = feature_extractor.predict(frames_batch[None, j, :])\n",
    "        frame_mask[i, :length] = 1  # 1 = not masked, 0 = masked\n",
    "\n",
    "    return frame_features, frame_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make predictions on a single video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sequence_prediction(seq_model, frames, true_label):\n",
    "    # Add batch dimension to the frames to match model's expected input shape\n",
    "    frames = np.expand_dims(frames, axis=0)  # Shape becomes (1, 20, 2048)\n",
    "    \n",
    "    frame_features, frame_mask = create_video_features_and_mask(frames)\n",
    "    \n",
    "    # Predict using the model\n",
    "    probabilities = seq_model.predict([frame_features, frame_mask])[0]\n",
    "        \n",
    "    print(\"\\nTop-5 predicted actions:\")\n",
    "    for i in np.argsort(probabilities)[::-1][:5]:\n",
    "        print(f\"  - {class_vocab[i]}: {probabilities[i] * 100:5.2f}%\")\n",
    "    \n",
    "    # Get the top-1 predicted label (highest probability)\n",
    "    predicted_index = np.argmax(probabilities)\n",
    "    predicted_label = class_vocab[predicted_index]\n",
    "\n",
    "    # Convert true_label to index to align with predicted index format\n",
    "    true_label_index = class_vocab.index(true_label)\n",
    "\n",
    "    # Prepare y_true and y_pred as binary arrays (1 for correct label, 0 for others)\n",
    "    y_true = np.zeros(len(class_vocab))\n",
    "    y_pred = np.zeros(len(class_vocab))\n",
    "    y_true[true_label_index] = 1\n",
    "    y_pred[predicted_index] = 1\n",
    " \n",
    "    return y_true, y_pred, predicted_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def sequence_prediction(seq_model, frames, true_label):\n",
    "#     frame_features, frame_mask = create_video_features_and_mask(frames)\n",
    "#     probabilities = seq_model.predict([frame_features, frame_mask])[0]\n",
    "        \n",
    "#     print(\"\\nTop-5 predicted actions:\")\n",
    "#     for i in np.argsort(probabilities)[::-1][:5]:\n",
    "#         print(f\"  - {class_vocab[i]}: {probabilities[i] * 100:5.2f}%\")\n",
    "    \n",
    "#     # Get the top-1 predicted label (highest probability)\n",
    "#     predicted_index = np.argmax(probabilities)\n",
    "#     predicted_label = class_vocab[predicted_index]\n",
    "\n",
    "#     # Convert true_label to index to align with predicted index format\n",
    "#     true_label_index = class_vocab.index(true_label)\n",
    "\n",
    "#     # Prepare y_true and y_pred as binary arrays (1 for correct label, 0 for others)\n",
    "#     y_true = np.zeros(len(class_vocab))\n",
    "#     y_pred = np.zeros(len(class_vocab))\n",
    "#     y_true[true_label_index] = 1\n",
    "#     y_pred[predicted_index] = 1\n",
    " \n",
    "#     return y_true, y_pred, predicted_label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display predicted image as GIF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_as_gif(images, model_version_path):\n",
    "    gif_path = f\"{model_version_path}/test_animation.gif\"\n",
    "    converted_images = images.astype(np.uint8)\n",
    "    imageio.mimsave(gif_path, converted_images, duration=100)\n",
    "    # return Image(gif_path)\n",
    "    return Image.open(gif_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choosing a random video to use for prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_index = np.random.randint(len(test_df))\n",
    "\n",
    "# Get the test video path\n",
    "test_video = test_df[\"video_path\"].values[random_index]\n",
    "\n",
    "# Get the true label of the test video\n",
    "true_label_index = test_labels.tolist()[random_index][0]\n",
    "true_label = class_vocab[true_label_index]\n",
    "\n",
    "test_video_frames = load_and_preprocess_video(test_video)\n",
    "\n",
    "print(f\"Test video path: {test_video}\")\n",
    "print(f\"Label: {true_label}\")\n",
    "\n",
    "print(f\"\"\"\n",
    "Test video frames shape:\n",
    "  - {test_video_frames.shape[0]} frames\n",
    "  - {test_video_frames.shape[1]} pixels (height) x {test_video_frames.shape[2]} pixels (width)\n",
    "  - {test_video_frames.shape[3]} color channels\n",
    "\"\"\")\n",
    "\n",
    "# display_as_gif(test_video_frames[:MAX_SEQ_LENGTH], model_version_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the prediction on the test video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_true, y_pred, predicted_label = sequence_prediction(sequence_model, test_video_frames, true_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(f\"\\nTrue label: {true_label}\")\n",
    "# print(f\"Predicted label: {predicted_label}\")\n",
    "\n",
    "# print(f\"y_true: {y_true}\")\n",
    "# print(f\"y_pred: {y_pred}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation on the entire test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utility to save classification metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_classification_report(report, save_path):\n",
    "    report_data = []\n",
    "    lines = report.split(\"\\n\")\n",
    "\n",
    "    for line in filter(None, lines[2:-3]):  # Remove empty lines and headers/footers\n",
    "        row = line.split()\n",
    "        name = row[0]\n",
    "        stats = row[1:]\n",
    "\n",
    "        # Convert stats to float, handling support as int\n",
    "        stats = [float(val) for val in stats[:-1]] + [int(stats[-1])]\n",
    "        report_data.append([name] + stats)\n",
    "\n",
    "    report_df = pd.DataFrame(\n",
    "        report_data, columns=[\"Class\", \"Precision\", \"Recall\", \"F1-score\", \"Support\"]\n",
    "    )\n",
    "\n",
    "    report_df.to_csv(save_path, index=False)\n",
    "\n",
    "    print(f\"Classification report saved at: {save_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utility to display evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_evaluation_metrics(y_true, y_pred, predictions, target_classes):\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    print(f\"Top-1 Accuracy: {accuracy * 100:.2f}%\")\n",
    "\n",
    "    k = 2\n",
    "    top_k_acc = top_k_accuracy_score(y_true, predictions, k=k)\n",
    "    print(f\"Top-{k} Accuracy: {top_k_acc * 100:.2f}%\")\n",
    "\n",
    "    report = classification_report(y_true, y_pred, target_names=target_classes)\n",
    "    print(\"\\nClassification Report:\\n\", report)\n",
    "\n",
    "    save_classification_report(report, f'{model_version_path}/classification_metrics.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = sequence_model.predict([test_features, test_masks], batch_size=32)\n",
    "y_pred = np.argmax(predictions, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_evaluation_metrics(test_labels, y_pred, predictions, class_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_confusion_matrix(y_true, y_pred, target_classes, plot_path, show_plot=False):\n",
    "    conf_matrix = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(\n",
    "        conf_matrix,\n",
    "        fmt=\"d\",\n",
    "        annot=True,\n",
    "        cmap=\"Blues\",\n",
    "        cbar=True,\n",
    "        xticklabels=target_classes,\n",
    "        yticklabels=target_classes,\n",
    "    )\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"True\")\n",
    "    plt.title(\"Confusion Matrix\")\n",
    "\n",
    "    if show_plot:\n",
    "        plt.show()    \n",
    "    else:\n",
    "        print(conf_matrix)\n",
    "\n",
    "    plt.savefig(plot_path)\n",
    "    print(f\"\\nConfusion matrix saved at: {plot_path}\")\n",
    "\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_labels = np.array(test_labels).flatten()\n",
    "cm_plot_path = get_model_image_path(\"confusion_matrix\", model_version_path)\n",
    "display_confusion_matrix(true_labels, y_pred, class_vocab, cm_plot_path, show_plot=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> K. Soomro, A. R. Zamir, and M. Shah, \"UCF101: A Dataset of 101 Human Actions Classes From Videos in The Wild,\" arXiv, 2012. [Online]. Available: https://arxiv.org/abs/1212.0402\n",
    "\n",
    "> A. Arnab, M. Dehghani, G. Heigold, C. Sun, M. Lučić, and C. Schmid, “ViVIT: a video vision transformer,” arXiv.org, Mar. 29, 2021. https://arxiv.org/abs/2103.15691\n",
    "\n",
    "> X. Liu, Y. Shen, J. Liu, J. Yang, P. Xiong, and F. Lin, “Parallel Spatial–Temporal Self-Attention CNN-Based Motor Imagery Classification for BCI,” Frontiers in Neuroscience, vol. 14, Dec. 2020, doi: 10.3389/fnins.2020.587520.\n",
    "\n",
    "> C. Szegedy, V. Vanhoucke, S. Ioffe, J. Shlens, and Z. Wojna, \"Rethinking the Inception Architecture for Computer Vision,\" arXiv preprint arXiv:1512.00567, 2015.\n",
    "\n",
    "> Singh, S., Dewangan, S., Krishna, G., Tyagi, V., & Reddy, S. (2022). Video vision transformers for violence detection. arXiv. https://doi.org/10.48550/arXiv.2209.03561"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 4915922,
     "datasetId": 2807884,
     "sourceId": 4849320,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30787,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
